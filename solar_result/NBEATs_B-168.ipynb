{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e097566-30f2-4f57-b3a2-2ae3b7d02005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 15:41:42.927097: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-04 15:41:43.003989: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-09-04 15:41:43.004007: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-09-04 15:41:43.373919: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-04 15:41:43.373973: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-04 15:41:43.373979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "#import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c74fd4-9d10-4b7d-9c83-4da3c7f1871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "target_X= pd.read_csv(\"../data/solor_train_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "target_y =pd.read_csv(\"../data/solor_train_output_7.csv\").iloc[:,1:].values\n",
    "test_X= pd.read_csv(\"../data/solor_val_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "test_y =pd.read_csv(\"../data/solor_val_output_7.csv\").iloc[:,1:].values\n",
    "\n",
    "X_train=target_X\n",
    "y_train=target_y\n",
    "\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475bd839-0f9e-49d3-96a8-f1fe23500d97",
   "metadata": {},
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117869af-8623-4129-aa35-bcc7ee3da674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],y_train.shape[1],1,1,32\n",
    "\n",
    "#################################################################################\n",
    "# nbeats + I모델 생성 함수\n",
    "def bulid_model(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK)\n",
    "                   ,nb_blocks_per_stack=1, thetas_dim=(1,2,4,4),\n",
    "                   share_weights_in_stack=True, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + G모델 생성 함수    \n",
    "def bulid_model_G(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.GENERIC_BLOCK,NBeatsKeras.GENERIC_BLOCK)\n",
    "                   ,nb_blocks_per_stack=5, thetas_dim=(4,4),\n",
    "                   share_weights_in_stack=False, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models_G(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model_G(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "# SMAPE 용\n",
    "def train_bagging_models_smape(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[np.newaxis, ...]\n",
    "\n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.pe[:, :tf.shape(x)[1], :]\n",
    "        return self.dropout(x)\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "def create_model(fn,d_model, nlayers, nhead, dropout, iw, ow,lr):\n",
    "    \n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(pretrained_output_reshaped)\n",
    "    x = layers.Dense(d_model, activation='relu')(x)\n",
    "    \n",
    "    pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "    x = pos_encoding(x)\n",
    "    \n",
    "    for _ in range(nlayers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model, dropout=dropout)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn_output = layers.Dense(d_model, activation='relu')(x)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "    \n",
    "    outputs = layers.Dense((iw + ow) // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(ow)(outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    target_model = Model(inputs=inputs, outputs=outputs)\n",
    "    target_model.compile(optimizer=optimizer, loss=fn)\n",
    "    \n",
    "    return target_model\n",
    "#################################################################################\n",
    "# 예측\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4c593-18a3-4fa1-be2e-8894dc7d453f",
   "metadata": {},
   "source": [
    "# 모형적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f13c2f-9812-460b-9269-22aa352bfc7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.4712 - val_loss: 0.8152\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.9022 - val_loss: 0.7445\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8251 - val_loss: 0.6640\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7915 - val_loss: 0.6606\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.7812 - val_loss: 0.6708\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7682 - val_loss: 0.6723\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7571 - val_loss: 0.6178\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7365 - val_loss: 0.6258\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7175 - val_loss: 0.6266\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7083 - val_loss: 0.6299\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7122 - val_loss: 0.6183\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6915 - val_loss: 0.6167\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6766 - val_loss: 0.6156\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6768 - val_loss: 0.6162\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6662 - val_loss: 0.6490\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6616 - val_loss: 0.6226\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6465 - val_loss: 0.6085\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.6237\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6492 - val_loss: 0.6157\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6274 - val_loss: 0.6316\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6333 - val_loss: 0.6338\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6271 - val_loss: 0.6162\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6146 - val_loss: 0.6248\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6433 - val_loss: 0.6324\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6376 - val_loss: 0.6525\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 0.6358\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.6338\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.3930 - val_loss: 0.7800\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8838 - val_loss: 0.7146\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8208 - val_loss: 0.7091\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7962 - val_loss: 0.7107\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7876 - val_loss: 0.6806\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7590 - val_loss: 0.6464\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7550 - val_loss: 0.6463\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7290 - val_loss: 0.6646\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7170 - val_loss: 0.6594\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7169 - val_loss: 0.6396\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6998 - val_loss: 0.6159\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6996 - val_loss: 0.6216\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7010 - val_loss: 0.6466\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6817 - val_loss: 0.6208\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6705 - val_loss: 0.6170\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6718 - val_loss: 0.6467\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6681 - val_loss: 0.6205\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.6122\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6600 - val_loss: 0.6314\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.6075\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6497 - val_loss: 0.6171\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6562 - val_loss: 0.6196\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6256 - val_loss: 0.6250\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6347 - val_loss: 0.6303\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6193 - val_loss: 0.6326\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6301 - val_loss: 0.6144\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6441 - val_loss: 0.6169\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6118 - val_loss: 0.6513\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6171 - val_loss: 0.6028\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6062 - val_loss: 0.6308\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5961 - val_loss: 0.6211\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5984 - val_loss: 0.6306\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5858 - val_loss: 0.6561\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5717 - val_loss: 0.6295\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.6219\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5659 - val_loss: 0.6548\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.6458\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5633 - val_loss: 0.6328\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5521 - val_loss: 0.6715\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 7ms/step - loss: 1.3951 - val_loss: 0.8085\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.9071 - val_loss: 0.7245\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8441 - val_loss: 0.6818\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8128 - val_loss: 0.6587\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7762 - val_loss: 0.6989\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7566 - val_loss: 0.6463\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7381 - val_loss: 0.6441\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7354 - val_loss: 0.6302\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7254 - val_loss: 0.6241\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7163 - val_loss: 0.6099\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7063 - val_loss: 0.6257\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6937 - val_loss: 0.6198\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7026 - val_loss: 0.6192\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7122 - val_loss: 0.6227\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6767 - val_loss: 0.6079\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6623 - val_loss: 0.6054\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6556 - val_loss: 0.6364\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6546 - val_loss: 0.6085\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.6817\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6611 - val_loss: 0.6068\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6369 - val_loss: 0.5883\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6276 - val_loss: 0.5938\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.6066\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6362 - val_loss: 0.6242\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6137 - val_loss: 0.6207\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6129 - val_loss: 0.6184\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5892\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5982 - val_loss: 0.6547\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5950 - val_loss: 0.6078\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5830\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.6034\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5784 - val_loss: 0.6125\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5663 - val_loss: 0.5966\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5566 - val_loss: 0.6199\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5812 - val_loss: 0.6116\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5680 - val_loss: 0.6011\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5386 - val_loss: 0.6059\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5434 - val_loss: 0.5922\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5183 - val_loss: 0.6000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5319 - val_loss: 0.6051\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.4055 - val_loss: 0.7783\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8870 - val_loss: 0.7758\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8275 - val_loss: 0.6855\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8046 - val_loss: 0.6613\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7739 - val_loss: 0.6348\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7631 - val_loss: 0.7185\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7404 - val_loss: 0.6411\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7366 - val_loss: 0.6591\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7179 - val_loss: 0.6298\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.7163 - val_loss: 0.6490\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7014 - val_loss: 0.6304\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7031 - val_loss: 0.6222\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6967 - val_loss: 0.6529\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6869 - val_loss: 0.6266\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6629 - val_loss: 0.6255\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6676 - val_loss: 0.6486\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6538 - val_loss: 0.6546\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6344 - val_loss: 0.6451\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6500 - val_loss: 0.5951\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6225 - val_loss: 0.6503\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6214 - val_loss: 0.6374\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6404 - val_loss: 0.6497\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6092 - val_loss: 0.7197\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6215 - val_loss: 0.6314\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5986 - val_loss: 0.6676\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 0.6482\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6027 - val_loss: 0.6638\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.6506\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.6668\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.3801 - val_loss: 0.7803\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8853 - val_loss: 0.7148\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8296 - val_loss: 0.6781\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7951 - val_loss: 0.6450\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7850 - val_loss: 0.6273\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7585 - val_loss: 0.6306\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7418 - val_loss: 0.6514\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7436 - val_loss: 0.6085\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7283 - val_loss: 0.6234\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7090 - val_loss: 0.5911\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6941 - val_loss: 0.6097\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6949 - val_loss: 0.6478\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7110 - val_loss: 0.6222\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6806 - val_loss: 0.6276\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6853 - val_loss: 0.6205\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6716 - val_loss: 0.5927\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6676 - val_loss: 0.6047\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6523 - val_loss: 0.6162\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6406 - val_loss: 0.6123\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6456 - val_loss: 0.6047\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.5320 - val_loss: 0.8452\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.9379 - val_loss: 0.7293\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8553 - val_loss: 0.7315\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8208 - val_loss: 0.6764\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7973 - val_loss: 0.6679\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7808 - val_loss: 0.7233\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7666 - val_loss: 0.6772\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7497 - val_loss: 0.6559\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7382 - val_loss: 0.6426\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7336 - val_loss: 0.6840\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7242 - val_loss: 0.6247\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6989 - val_loss: 0.6080\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 0.6868\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6990 - val_loss: 0.6104\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6831 - val_loss: 0.6262\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6746 - val_loss: 0.6274\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6939 - val_loss: 0.6611\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6648 - val_loss: 0.6408\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6611 - val_loss: 0.6158\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6355 - val_loss: 0.6413\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6324 - val_loss: 0.6129\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6361 - val_loss: 0.6588\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 1.5289 - val_loss: 0.8571\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.9160 - val_loss: 0.7625\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8552 - val_loss: 0.7251\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8240 - val_loss: 0.6941\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8060 - val_loss: 0.6862\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7919 - val_loss: 0.6696\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7780 - val_loss: 0.6556\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7567 - val_loss: 0.6541\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7785 - val_loss: 0.6563\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7342 - val_loss: 0.6482\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7179 - val_loss: 0.7436\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7352 - val_loss: 0.6638\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7225 - val_loss: 0.6137\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7125 - val_loss: 0.5970\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6824 - val_loss: 0.6225\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6810 - val_loss: 0.6278\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6831 - val_loss: 0.6236\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 0.6219\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6760 - val_loss: 0.6178\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6593 - val_loss: 0.6207\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6618 - val_loss: 0.6131\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6600 - val_loss: 0.6243\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.5966\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6356 - val_loss: 0.5840\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6289 - val_loss: 0.6478\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6360 - val_loss: 0.6353\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6207 - val_loss: 0.6074\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6055 - val_loss: 0.6203\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6057 - val_loss: 0.6298\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6146 - val_loss: 0.6161\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6175 - val_loss: 0.6133\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5854 - val_loss: 0.6183\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5886 - val_loss: 0.6397\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5677 - val_loss: 0.6447\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.3464 - val_loss: 0.7935\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8856 - val_loss: 0.7027\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8389 - val_loss: 0.6892\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8189 - val_loss: 0.6699\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7993 - val_loss: 0.6505\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7745 - val_loss: 0.6937\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7538 - val_loss: 0.6265\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7442 - val_loss: 0.6043\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7367 - val_loss: 0.6206\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7101 - val_loss: 0.6408\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7103 - val_loss: 0.6305\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6993 - val_loss: 0.6359\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6923 - val_loss: 0.6208\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6657 - val_loss: 0.6254\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7010 - val_loss: 0.6465\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6911 - val_loss: 0.6304\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6592 - val_loss: 0.6144\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6456 - val_loss: 0.6381\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 1.4606 - val_loss: 0.7929\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8898 - val_loss: 0.6973\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8264 - val_loss: 0.6823\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8146 - val_loss: 0.6619\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7727 - val_loss: 0.6498\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7768 - val_loss: 0.6465\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7748 - val_loss: 0.6501\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7559 - val_loss: 0.6337\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7344 - val_loss: 0.6241\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7131 - val_loss: 0.6103\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7090 - val_loss: 0.6326\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7097 - val_loss: 0.6059\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7030 - val_loss: 0.6245\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.5944\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6806 - val_loss: 0.6247\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6825 - val_loss: 0.6228\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6755 - val_loss: 0.6228\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6574 - val_loss: 0.6160\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6849 - val_loss: 0.5955\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6498 - val_loss: 0.6691\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6386 - val_loss: 0.6247\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6556 - val_loss: 0.6062\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6312 - val_loss: 0.5998\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6392 - val_loss: 0.6145\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.6552 - val_loss: 0.8834\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.9237 - val_loss: 0.7873\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8725 - val_loss: 0.7262\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8119 - val_loss: 0.7058\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7870 - val_loss: 0.6782\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7598 - val_loss: 0.6710\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7578 - val_loss: 0.6553\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7246 - val_loss: 0.6627\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7331 - val_loss: 0.6522\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7190 - val_loss: 0.6485\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7219 - val_loss: 0.6226\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6936 - val_loss: 0.6591\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7052 - val_loss: 0.6318\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6856 - val_loss: 0.6270\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6696 - val_loss: 0.6101\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6581 - val_loss: 0.6373\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6605 - val_loss: 0.6228\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6593 - val_loss: 0.6231\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6461 - val_loss: 0.6904\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6291 - val_loss: 0.6730\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6304 - val_loss: 0.6305\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6230 - val_loss: 0.6400\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6275 - val_loss: 0.6796\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6122 - val_loss: 0.6335\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6051 - val_loss: 0.6392\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 7ms/step - loss: 124481168.0000 - val_loss: 36750968.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 28216514.0000 - val_loss: 23447498.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 18615666.0000 - val_loss: 13878170.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10536122.0000 - val_loss: 7017214.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6287074.5000 - val_loss: 4840567.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4095217.2500 - val_loss: 2944243.7500\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3042693.7500 - val_loss: 2271127.7500\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2339014.7500 - val_loss: 2042196.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1697504.1250 - val_loss: 1533674.3750\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1239672.6250 - val_loss: 1061071.1250\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 957051.8750 - val_loss: 806255.3125\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1127620.1250 - val_loss: 1415100.7500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 811891.3125 - val_loss: 680508.8125\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 554525.6875 - val_loss: 660441.1875\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 660994.0000 - val_loss: 729351.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 554209.0625 - val_loss: 520631.3438\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 434738.5625 - val_loss: 513106.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 413440.9062 - val_loss: 396231.6250\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 289300.3438 - val_loss: 974752.8750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 345096.4688 - val_loss: 255074.5938\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 333737.6875 - val_loss: 677455.3750\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 299394.5312 - val_loss: 247386.8906\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 241100.2500 - val_loss: 279252.5312\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 192203.7656 - val_loss: 439277.3750\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 199761.8125 - val_loss: 182544.3281\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 161483.2031 - val_loss: 538618.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 256648.2812 - val_loss: 170883.7031\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 185581.0312 - val_loss: 239435.5625\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 179967.6406 - val_loss: 413629.0938\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 306215.3438 - val_loss: 565115.8750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 175152.4844 - val_loss: 309177.0000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 162719.1719 - val_loss: 247644.0781\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 109730.2031 - val_loss: 205758.3281\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 185141.1875 - val_loss: 525399.8125\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 198188.3125 - val_loss: 202707.1250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 173184.5000 - val_loss: 200665.1562\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 247922.3125 - val_loss: 241748.7969\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 128971888.0000 - val_loss: 42731708.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 31311224.0000 - val_loss: 24312520.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 16946596.0000 - val_loss: 12975197.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8778201.0000 - val_loss: 6163440.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5108118.0000 - val_loss: 4069918.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3701240.5000 - val_loss: 3644536.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2252518.7500 - val_loss: 2464885.2500\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1848470.2500 - val_loss: 1995167.1250\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1320391.2500 - val_loss: 1616643.2500\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1419632.7500 - val_loss: 1476041.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1259053.3750 - val_loss: 2269768.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1172133.8750 - val_loss: 1294781.6250\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 880527.0625 - val_loss: 1419393.3750\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 821912.9375 - val_loss: 998057.3750\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 777314.4375 - val_loss: 1185995.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 937851.1250 - val_loss: 1165534.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 752524.2500 - val_loss: 1212965.2500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 589794.8750 - val_loss: 719734.4375\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 641263.0000 - val_loss: 1424170.6250\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 591731.4375 - val_loss: 1626231.0000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 619023.2500 - val_loss: 1130728.6250\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 424144.7812 - val_loss: 715373.9375\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 449424.1875 - val_loss: 644797.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 348867.9062 - val_loss: 828825.0000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 342636.1875 - val_loss: 661654.1875\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 288282.3125 - val_loss: 641990.8125\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 366403.1875 - val_loss: 700096.0625\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 362075.1250 - val_loss: 709369.6250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 350184.6562 - val_loss: 1096109.7500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 333829.4375 - val_loss: 560104.8125\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 322014.5625 - val_loss: 524160.3438\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 277223.7812 - val_loss: 505769.7500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 335681.0000 - val_loss: 742278.6875\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 288643.8750 - val_loss: 729123.5000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 232501.1406 - val_loss: 638977.3750\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 278282.4375 - val_loss: 597191.7500\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 377873.9062 - val_loss: 544917.8125\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 227501.6719 - val_loss: 580318.5625\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 175861.0312 - val_loss: 500764.1875\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 203663.1875 - val_loss: 507720.5625\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 152435.3594 - val_loss: 593404.9375\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 185741.5938 - val_loss: 719622.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 168171.9688 - val_loss: 501902.1875\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 149450.0625 - val_loss: 462001.9688\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 195232.4375 - val_loss: 462133.1875\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 220714.0469 - val_loss: 600718.8750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 226544.9688 - val_loss: 477935.4062\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 149055.1250 - val_loss: 583337.2500\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 174083.4375 - val_loss: 577832.5625\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 145319.3125 - val_loss: 531888.2500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 164643.0469 - val_loss: 482999.7188\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 217729.3438 - val_loss: 677147.5000\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 157509.3281 - val_loss: 488932.0938\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 244000.6406 - val_loss: 538327.8750\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 112015136.0000 - val_loss: 52271992.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 33486824.0000 - val_loss: 27369570.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 21393236.0000 - val_loss: 22643946.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 17061170.0000 - val_loss: 20430840.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 13188946.0000 - val_loss: 15204777.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 9892850.0000 - val_loss: 10083107.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8279616.0000 - val_loss: 9759902.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6654902.0000 - val_loss: 7344519.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5583054.0000 - val_loss: 7149035.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5001525.5000 - val_loss: 6008421.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4321306.0000 - val_loss: 5364641.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4745397.5000 - val_loss: 5054877.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3851453.7500 - val_loss: 4339268.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3362881.7500 - val_loss: 3902480.7500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2954585.0000 - val_loss: 3271751.2500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2876388.2500 - val_loss: 3584704.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2382578.7500 - val_loss: 2782277.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2306609.0000 - val_loss: 3043731.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1849500.1250 - val_loss: 2046150.8750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1905366.6250 - val_loss: 3517689.0000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2056257.7500 - val_loss: 1919246.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1739343.3750 - val_loss: 2442771.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1408299.5000 - val_loss: 2609916.5000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1434527.5000 - val_loss: 2387150.0000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1319861.8750 - val_loss: 1725445.5000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1268546.7500 - val_loss: 1362800.0000\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1301229.8750 - val_loss: 1404634.1250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 942495.5625 - val_loss: 1672178.8750\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1126119.6250 - val_loss: 1429812.3750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 963197.2500 - val_loss: 1039500.3750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1248689.3750 - val_loss: 967091.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 767737.2500 - val_loss: 831713.3125\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 646636.8750 - val_loss: 1094323.6250\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 769183.6875 - val_loss: 662724.3125\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 415985.3438 - val_loss: 560933.4375\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 662027.0000 - val_loss: 1044273.1875\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 485056.1250 - val_loss: 439657.0625\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 491016.1562 - val_loss: 488595.1562\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 376067.7500 - val_loss: 305216.3438\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 322787.8438 - val_loss: 439113.6250\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 385339.9062 - val_loss: 362633.0000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 385096.5000 - val_loss: 386577.0625\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 418874.9375 - val_loss: 324984.3438\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 201981.0625 - val_loss: 264636.3125\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 328045.6250 - val_loss: 891891.9375\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 231854.5469 - val_loss: 206748.3438\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 320593.6250 - val_loss: 233408.5781\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 207587.5312 - val_loss: 460168.2188\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 286420.1562 - val_loss: 467907.8750\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 292582.3750 - val_loss: 408587.0312\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 193004.7344 - val_loss: 194056.9688\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 224254.5625 - val_loss: 311556.4062\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 205112.8125 - val_loss: 502617.7500\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 271969.5625 - val_loss: 843840.9375\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 255079.3125 - val_loss: 314939.9375\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 194463.2188 - val_loss: 198503.3594\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 174126.1562 - val_loss: 416835.7812\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 236495.5312 - val_loss: 208783.2812\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 154083.8125 - val_loss: 181023.1406\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 193895.4531 - val_loss: 456483.0938\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 238454.5000 - val_loss: 582246.6250\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 233993.1406 - val_loss: 437055.6250\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 224178.5469 - val_loss: 288268.2188\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 271784.7812 - val_loss: 387259.0312\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 268834.6250 - val_loss: 219068.4062\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125222.4062 - val_loss: 217030.0625\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 182519.1719 - val_loss: 150877.2969\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131005.9062 - val_loss: 508394.3750\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 220685.6875 - val_loss: 180682.7500\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121140.1406 - val_loss: 203751.3750\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 119136.7188 - val_loss: 187944.5469\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 193664.5781 - val_loss: 579880.0000\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122457.7812 - val_loss: 209064.3281\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134846.4375 - val_loss: 265658.5000\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 105287.2812 - val_loss: 302887.4062\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123862.7891 - val_loss: 286676.8125\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 118471.1641 - val_loss: 172530.0938\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 6ms/step - loss: 124268616.0000 - val_loss: 44361868.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 34526840.0000 - val_loss: 25102490.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 22016776.0000 - val_loss: 18836716.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 15369410.0000 - val_loss: 16606734.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 12144996.0000 - val_loss: 13709177.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10115192.0000 - val_loss: 10029448.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8838406.0000 - val_loss: 8447515.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7949074.5000 - val_loss: 7161495.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6110150.0000 - val_loss: 6621540.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5860177.5000 - val_loss: 6000601.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6104905.0000 - val_loss: 5892323.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4985132.5000 - val_loss: 5563853.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4462641.5000 - val_loss: 4847407.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4898409.0000 - val_loss: 4887779.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4220408.5000 - val_loss: 7094164.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3673901.7500 - val_loss: 5118731.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3553955.0000 - val_loss: 4237344.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3055728.5000 - val_loss: 4779002.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3402031.7500 - val_loss: 3490052.5000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2978551.0000 - val_loss: 2814828.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3001295.0000 - val_loss: 2863945.2500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2767460.0000 - val_loss: 2865885.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2791680.2500 - val_loss: 3181739.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2402339.5000 - val_loss: 3401498.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 2223564.5000 - val_loss: 2287115.7500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2336997.5000 - val_loss: 3172550.7500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2788741.0000 - val_loss: 1916805.1250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2355003.0000 - val_loss: 3221518.0000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2431436.0000 - val_loss: 1946941.3750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1671075.6250 - val_loss: 1932747.0000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1867562.2500 - val_loss: 2566648.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1722424.2500 - val_loss: 1712261.5000\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1968919.1250 - val_loss: 2194242.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1798437.6250 - val_loss: 1724632.1250\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1529395.2500 - val_loss: 2550310.7500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1744153.1250 - val_loss: 1669759.1250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1634812.5000 - val_loss: 1580020.0000\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1191074.2500 - val_loss: 1449507.1250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1205372.5000 - val_loss: 1247189.6250\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1112551.3750 - val_loss: 1753905.3750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1330853.6250 - val_loss: 1512680.0000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1209199.1250 - val_loss: 1341527.8750\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1416676.0000 - val_loss: 2468037.0000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1004876.7500 - val_loss: 1281784.1250\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1130006.3750 - val_loss: 1727703.7500\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1118287.5000 - val_loss: 1557457.7500\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1209712.8750 - val_loss: 1197813.1250\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1094500.2500 - val_loss: 1302396.8750\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 966278.0000 - val_loss: 1031906.4375\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1069117.7500 - val_loss: 1763358.8750\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1072084.5000 - val_loss: 1383021.6250\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1276910.3750 - val_loss: 1476251.3750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1180677.7500 - val_loss: 1193116.1250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1145941.6250 - val_loss: 1201641.2500\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1313210.0000 - val_loss: 1202928.3750\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1206328.6250 - val_loss: 1210928.7500\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1090298.0000 - val_loss: 1262072.1250\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1262610.5000 - val_loss: 949517.8125\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 907049.6875 - val_loss: 1097402.3750\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1109562.2500 - val_loss: 874123.4375\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 917922.1875 - val_loss: 1379003.6250\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 736695.8750 - val_loss: 989340.5000\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 882962.4375 - val_loss: 831710.8750\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 703667.2500 - val_loss: 1282575.0000\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 723149.4375 - val_loss: 1009539.5000\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 881211.8750 - val_loss: 792070.6875\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 625789.1250 - val_loss: 754872.4375\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1055098.7500 - val_loss: 864986.6875\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 775831.4375 - val_loss: 776074.8125\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 572848.7500 - val_loss: 861231.2500\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 820915.7500 - val_loss: 767116.6250\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 609902.5000 - val_loss: 751082.6875\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 810861.6875 - val_loss: 1053625.5000\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 639665.6875 - val_loss: 591343.6875\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 606674.0625 - val_loss: 598344.4375\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 637817.2500 - val_loss: 692398.8125\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 510904.7188 - val_loss: 1022138.8125\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 620563.5000 - val_loss: 620693.4375\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 478014.7812 - val_loss: 455759.7812\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 558386.2500 - val_loss: 870716.6875\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 627644.1875 - val_loss: 1134201.0000\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 516059.4375 - val_loss: 714963.6875\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 609852.5625 - val_loss: 542023.8125\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 566502.1875 - val_loss: 645857.3125\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 522642.5625 - val_loss: 700667.8125\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 523867.6562 - val_loss: 609988.8125\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 463503.9375 - val_loss: 1071607.3750\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 691743.1250 - val_loss: 889558.5625\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 521983.0625 - val_loss: 330384.4688\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 397156.4688 - val_loss: 486824.0000\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 407768.6250 - val_loss: 520944.2812\n",
      "Epoch 92/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 509574.8750 - val_loss: 755968.8750\n",
      "Epoch 93/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 385564.9375 - val_loss: 478512.4375\n",
      "Epoch 94/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 375697.0625 - val_loss: 455541.4062\n",
      "Epoch 95/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 325125.1250 - val_loss: 472905.8750\n",
      "Epoch 96/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 448528.2188 - val_loss: 317936.1250\n",
      "Epoch 97/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 345493.8125 - val_loss: 289660.5938\n",
      "Epoch 98/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 334237.5938 - val_loss: 260099.0312\n",
      "Epoch 99/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 301665.0312 - val_loss: 291674.9688\n",
      "Epoch 100/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 304415.0938 - val_loss: 330999.8125\n",
      "Epoch 101/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 232998.9062 - val_loss: 252056.0781\n",
      "Epoch 102/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 289066.9375 - val_loss: 270361.6875\n",
      "Epoch 103/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 277344.3438 - val_loss: 802217.3125\n",
      "Epoch 104/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 261079.3281 - val_loss: 188488.2188\n",
      "Epoch 105/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 221658.7500 - val_loss: 334446.7188\n",
      "Epoch 106/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 200032.8125 - val_loss: 223003.0781\n",
      "Epoch 107/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 211844.7188 - val_loss: 249082.1562\n",
      "Epoch 108/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 228146.4375 - val_loss: 247143.7188\n",
      "Epoch 109/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 205517.6719 - val_loss: 280237.1875\n",
      "Epoch 110/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 248728.4688 - val_loss: 201567.5000\n",
      "Epoch 111/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 291292.1250 - val_loss: 340662.9062\n",
      "Epoch 112/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 235674.3281 - val_loss: 206194.0625\n",
      "Epoch 113/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 223919.1875 - val_loss: 333519.9688\n",
      "Epoch 114/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 213720.7344 - val_loss: 160302.8281\n",
      "Epoch 115/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 231452.5469 - val_loss: 417035.9375\n",
      "Epoch 116/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 211929.6875 - val_loss: 180284.8750\n",
      "Epoch 117/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 171389.1094 - val_loss: 259229.2969\n",
      "Epoch 118/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 232654.5312 - val_loss: 178047.2969\n",
      "Epoch 119/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 138546.6875 - val_loss: 141592.3281\n",
      "Epoch 120/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 171241.1719 - val_loss: 244666.2969\n",
      "Epoch 121/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 162475.3750 - val_loss: 420432.7500\n",
      "Epoch 122/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 213178.7031 - val_loss: 302450.1250\n",
      "Epoch 123/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136442.7188 - val_loss: 123340.2891\n",
      "Epoch 124/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 168490.9219 - val_loss: 223145.1250\n",
      "Epoch 125/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 197726.6406 - val_loss: 481762.2188\n",
      "Epoch 126/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 204756.1875 - val_loss: 138383.5781\n",
      "Epoch 127/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 175327.0938 - val_loss: 160888.4375\n",
      "Epoch 128/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 147407.7344 - val_loss: 121537.7969\n",
      "Epoch 129/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 144064.8125 - val_loss: 179905.2031\n",
      "Epoch 130/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 204171.2969 - val_loss: 322800.2500\n",
      "Epoch 131/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 193675.2656 - val_loss: 275994.1875\n",
      "Epoch 132/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141718.6875 - val_loss: 280810.2500\n",
      "Epoch 133/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 163750.7031 - val_loss: 191166.6562\n",
      "Epoch 134/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131648.8594 - val_loss: 120179.4922\n",
      "Epoch 135/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 172420.4531 - val_loss: 251125.6875\n",
      "Epoch 136/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 146668.3438 - val_loss: 168028.0938\n",
      "Epoch 137/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 174572.6875 - val_loss: 470483.5938\n",
      "Epoch 138/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 159063.9688 - val_loss: 107619.3203\n",
      "Epoch 139/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 153125.7188 - val_loss: 98273.1562\n",
      "Epoch 140/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 118816.1641 - val_loss: 174824.6250\n",
      "Epoch 141/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120773.9453 - val_loss: 122692.8438\n",
      "Epoch 142/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 97240.3906 - val_loss: 112856.2812\n",
      "Epoch 143/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 98025.2109 - val_loss: 68977.3594\n",
      "Epoch 144/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 236907.0312 - val_loss: 76054.5000\n",
      "Epoch 145/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 131094.7188 - val_loss: 272033.1250\n",
      "Epoch 146/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128231.4844 - val_loss: 76276.0703\n",
      "Epoch 147/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130022.8359 - val_loss: 165473.5469\n",
      "Epoch 148/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 110910.9062 - val_loss: 90842.1250\n",
      "Epoch 149/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 115387.8750 - val_loss: 238182.9844\n",
      "Epoch 150/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121431.9141 - val_loss: 75319.0000\n",
      "Epoch 151/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 90242.1094 - val_loss: 129331.8516\n",
      "Epoch 152/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 157936.6250 - val_loss: 72213.3594\n",
      "Epoch 153/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120392.8594 - val_loss: 139465.6094\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 104998096.0000 - val_loss: 38373280.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 27337076.0000 - val_loss: 26174878.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 16012075.0000 - val_loss: 13093999.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 11526402.0000 - val_loss: 12337762.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8999771.0000 - val_loss: 7394018.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6832899.0000 - val_loss: 8463427.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5938138.0000 - val_loss: 5815719.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4530930.5000 - val_loss: 4699391.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4611835.0000 - val_loss: 4112650.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3917029.2500 - val_loss: 3813561.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4030127.5000 - val_loss: 3937670.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3279029.5000 - val_loss: 3182958.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2760463.5000 - val_loss: 2468923.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2600156.5000 - val_loss: 2543790.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2563122.2500 - val_loss: 2809595.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2931888.0000 - val_loss: 2418982.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1953416.8750 - val_loss: 1879821.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1915736.5000 - val_loss: 2603867.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1665438.0000 - val_loss: 1537758.8750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1294195.1250 - val_loss: 1876094.0000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1276425.1250 - val_loss: 1511845.6250\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1310830.0000 - val_loss: 1817291.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1186562.2500 - val_loss: 1015172.0625\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1493762.3750 - val_loss: 1779065.2500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 983825.0000 - val_loss: 1564721.8750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 873809.5000 - val_loss: 2081241.3750\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 715699.8125 - val_loss: 714030.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 772900.4375 - val_loss: 676072.8750\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 707405.7500 - val_loss: 700333.0625\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 456380.8750 - val_loss: 931491.0000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 492061.9375 - val_loss: 493704.3438\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 648766.6250 - val_loss: 333840.3750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 411942.4375 - val_loss: 970356.0000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 383443.2812 - val_loss: 366051.3125\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 269191.0312 - val_loss: 515131.5938\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 433154.2188 - val_loss: 495926.9062\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 373166.4062 - val_loss: 661859.6875\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 250880.3906 - val_loss: 300899.0938\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 375468.5938 - val_loss: 498278.4688\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 374913.8750 - val_loss: 462153.3750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 368553.7500 - val_loss: 295182.0312\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 357545.9688 - val_loss: 413242.1562\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 402750.4375 - val_loss: 1212972.0000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 515006.8438 - val_loss: 440293.5312\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 388830.5312 - val_loss: 955439.4375\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 404810.2188 - val_loss: 197321.1406\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 320954.5000 - val_loss: 320351.6562\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 261017.3594 - val_loss: 214639.3594\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 482649.0625 - val_loss: 755777.5625\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 320107.0312 - val_loss: 342414.9375\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 228774.3594 - val_loss: 175055.8750\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 484046.6562 - val_loss: 490875.3125\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 255337.6094 - val_loss: 258334.7031\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 246848.1406 - val_loss: 550174.6875\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 354055.5312 - val_loss: 592834.6250\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 283942.1562 - val_loss: 204720.4844\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 245678.6094 - val_loss: 276788.4688\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 248849.3125 - val_loss: 353209.2188\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 260581.6875 - val_loss: 225352.5000\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 257880.3125 - val_loss: 248794.2656\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 295179.0938 - val_loss: 385709.3125\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 78005704.0000 - val_loss: 33812596.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 23406082.0000 - val_loss: 16472352.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 13619512.0000 - val_loss: 11137354.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9970184.0000 - val_loss: 8702876.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8069898.5000 - val_loss: 6882992.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7002982.5000 - val_loss: 6998983.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5277762.0000 - val_loss: 5447957.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4770999.5000 - val_loss: 4313706.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4460184.5000 - val_loss: 4938775.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5088563.5000 - val_loss: 6254335.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3716757.2500 - val_loss: 3258255.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3037883.0000 - val_loss: 4051131.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2783988.2500 - val_loss: 3286619.2500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2962416.5000 - val_loss: 2374542.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2368097.0000 - val_loss: 3802415.2500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2196934.0000 - val_loss: 2163553.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1868020.6250 - val_loss: 2320675.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2123245.7500 - val_loss: 3666947.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1770247.5000 - val_loss: 2220430.7500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1567033.7500 - val_loss: 1698385.5000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1614190.1250 - val_loss: 2703522.5000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1529659.6250 - val_loss: 1578770.2500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1377359.6250 - val_loss: 1568860.2500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1468391.5000 - val_loss: 2173241.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1274713.1250 - val_loss: 1978967.8750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1116448.0000 - val_loss: 1459398.8750\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1004210.1250 - val_loss: 1720599.2500\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1034367.2500 - val_loss: 1268571.2500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 930727.7500 - val_loss: 2478303.0000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1002199.0000 - val_loss: 1166420.2500\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1846546.6250 - val_loss: 1835303.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1053284.2500 - val_loss: 2405975.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1255560.8750 - val_loss: 1117646.2500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 907026.5000 - val_loss: 1288315.7500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 981042.6875 - val_loss: 1088458.8750\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 890221.0625 - val_loss: 935674.0625\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 816304.4375 - val_loss: 998078.6875\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 932787.4375 - val_loss: 982113.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1063240.3750 - val_loss: 1415205.6250\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 1132774.0000 - val_loss: 1088056.1250\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 866164.8750 - val_loss: 808150.8125\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 644566.6875 - val_loss: 737607.6250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 593116.4375 - val_loss: 681889.3125\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 800041.6875 - val_loss: 1190633.3750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 791418.1250 - val_loss: 816026.8750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 654128.0000 - val_loss: 846089.6875\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 691559.4375 - val_loss: 669928.2500\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 555057.5000 - val_loss: 955830.8125\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 561407.3750 - val_loss: 534479.3125\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 486364.5625 - val_loss: 584744.2500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 603694.4375 - val_loss: 750498.1875\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 609401.6875 - val_loss: 713827.0000\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 410094.8750 - val_loss: 720556.5000\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 368418.2188 - val_loss: 462302.3125\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 432267.6250 - val_loss: 685194.1250\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 307405.3750 - val_loss: 520958.9375\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 296169.3750 - val_loss: 542892.0000\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 242008.3125 - val_loss: 352046.4688\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 256403.6875 - val_loss: 458068.0938\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 278398.0938 - val_loss: 461955.8125\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 260258.5000 - val_loss: 336821.0938\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 251174.6406 - val_loss: 308754.0312\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 253853.8906 - val_loss: 394460.8750\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 273864.3750 - val_loss: 388734.9688\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 176462.3125 - val_loss: 406537.7188\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 277244.2812 - val_loss: 455505.6250\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 232575.1562 - val_loss: 879373.8125\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 275544.2188 - val_loss: 307176.8438\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 185505.1094 - val_loss: 330835.1562\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 266075.4062 - val_loss: 340653.8438\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 202539.1250 - val_loss: 309979.4688\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 178541.3906 - val_loss: 545251.3125\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 219698.3750 - val_loss: 464857.1562\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 283567.0312 - val_loss: 588532.1250\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 221650.9844 - val_loss: 295098.3125\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 185306.5625 - val_loss: 293518.1562\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 210296.3125 - val_loss: 574607.3125\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 206138.4844 - val_loss: 417837.1875\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 152551.7188 - val_loss: 253548.8750\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 242363.3281 - val_loss: 366732.8125\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 214011.8281 - val_loss: 288253.0625\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 135994.9531 - val_loss: 261344.3906\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 207015.9375 - val_loss: 570534.8125\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 229771.7812 - val_loss: 393031.1875\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 212744.8594 - val_loss: 392856.6250\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 179669.9688 - val_loss: 422326.7188\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 337123.3125 - val_loss: 572709.7500\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 243287.3906 - val_loss: 343806.5000\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 176068.9219 - val_loss: 295968.9688\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 145662400.0000 - val_loss: 40769188.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 28524594.0000 - val_loss: 21185400.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 17103694.0000 - val_loss: 15085208.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12326319.0000 - val_loss: 11553278.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9784089.0000 - val_loss: 8885192.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8237421.5000 - val_loss: 8021213.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6844447.5000 - val_loss: 5750521.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4301291.5000 - val_loss: 4277400.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3006173.5000 - val_loss: 3448417.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2440143.7500 - val_loss: 2335513.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1729234.2500 - val_loss: 1796552.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1545324.1250 - val_loss: 1748856.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1181027.6250 - val_loss: 1591038.2500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1169966.3750 - val_loss: 1460029.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 774264.1250 - val_loss: 1504191.3750\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 885253.2500 - val_loss: 859320.6875\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 822511.5625 - val_loss: 959935.7500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 888685.8750 - val_loss: 2237198.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 697628.1250 - val_loss: 725842.5000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 720833.2500 - val_loss: 917240.3750\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 630197.8125 - val_loss: 1017033.0625\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 722542.3750 - val_loss: 944064.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 687982.7500 - val_loss: 1351924.2500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 401742.8750 - val_loss: 877981.6875\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 401353.6250 - val_loss: 1095908.1250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 444608.8125 - val_loss: 563609.6250\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 477505.2188 - val_loss: 532364.9375\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 451236.8750 - val_loss: 434650.0938\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 359848.8750 - val_loss: 858352.6875\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 330065.8438 - val_loss: 864544.6875\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 336705.0938 - val_loss: 774955.1250\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 372155.8750 - val_loss: 789758.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 257245.1719 - val_loss: 382132.3438\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 271450.2188 - val_loss: 552943.7500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 240261.2188 - val_loss: 511746.1562\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 252004.2188 - val_loss: 833097.9375\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 269449.1875 - val_loss: 418281.6562\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 238377.1406 - val_loss: 566276.9375\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 299870.5625 - val_loss: 402211.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 484057.7188 - val_loss: 551576.5625\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 229667.6094 - val_loss: 383810.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 235848.2812 - val_loss: 1007169.7500\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 308531.8750 - val_loss: 1041724.5625\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 118671144.0000 - val_loss: 54996072.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 33441464.0000 - val_loss: 24463738.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 20713750.0000 - val_loss: 17710232.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 14004344.0000 - val_loss: 13377737.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10255204.0000 - val_loss: 9614626.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8853921.0000 - val_loss: 14778016.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8560380.0000 - val_loss: 6889700.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6036263.0000 - val_loss: 6215914.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6478197.0000 - val_loss: 10509757.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6583342.5000 - val_loss: 5722400.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4370312.0000 - val_loss: 4600976.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4234908.0000 - val_loss: 4807335.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3711345.5000 - val_loss: 3638001.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2897112.2500 - val_loss: 3540102.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2895340.0000 - val_loss: 3729559.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2928457.7500 - val_loss: 4399151.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2376113.0000 - val_loss: 2691637.2500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2439482.0000 - val_loss: 2835636.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2631699.0000 - val_loss: 2934625.0000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2196364.0000 - val_loss: 2396539.0000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1854034.1250 - val_loss: 2051275.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1784982.5000 - val_loss: 2546245.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1580564.7500 - val_loss: 2185800.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1688864.2500 - val_loss: 3082318.2500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1978520.6250 - val_loss: 2131188.2500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1684912.0000 - val_loss: 1929417.0000\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1268736.6250 - val_loss: 1478063.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1498279.2500 - val_loss: 3364559.7500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1508373.5000 - val_loss: 1315117.6250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1390182.6250 - val_loss: 1932490.3750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1111821.3750 - val_loss: 1243776.3750\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1222629.0000 - val_loss: 1086144.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 973726.7500 - val_loss: 2182225.5000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1369483.6250 - val_loss: 1642159.2500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1195765.2500 - val_loss: 919738.1875\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 792185.5625 - val_loss: 833088.1250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 951304.5625 - val_loss: 1827403.8750\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 864491.8125 - val_loss: 856709.2500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1023782.1250 - val_loss: 1042821.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 686980.3750 - val_loss: 1034377.2500\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 720240.7500 - val_loss: 771469.8750\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 620413.8750 - val_loss: 934416.4375\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 629569.8125 - val_loss: 598402.3125\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 881342.7500 - val_loss: 982718.0000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 735687.4375 - val_loss: 744040.1250\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 575785.3125 - val_loss: 660323.3750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 702297.0625 - val_loss: 712718.9375\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 517738.9375 - val_loss: 932243.6250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 592006.3750 - val_loss: 560967.2500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 695913.5000 - val_loss: 485124.6875\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 549852.6875 - val_loss: 542535.3125\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 497108.0000 - val_loss: 466685.7812\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 458031.2500 - val_loss: 489010.3125\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 423206.5938 - val_loss: 479989.3438\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 427388.3750 - val_loss: 773699.0625\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 528680.1875 - val_loss: 506945.8750\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 344738.5938 - val_loss: 453202.4375\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 327721.0938 - val_loss: 405944.5938\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 324223.7812 - val_loss: 425590.9062\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 384736.2188 - val_loss: 491231.0625\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 316142.5625 - val_loss: 315169.6562\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 320354.8750 - val_loss: 409487.6875\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 280052.4375 - val_loss: 584325.8125\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 286430.8125 - val_loss: 279794.9375\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 302302.1250 - val_loss: 449955.1250\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 301855.6250 - val_loss: 354665.8125\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 314454.8750 - val_loss: 596090.9375\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 318225.8750 - val_loss: 336265.5312\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 259986.9375 - val_loss: 376221.5938\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 275072.5938 - val_loss: 554914.0625\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 250645.7812 - val_loss: 330532.0938\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 220665.2812 - val_loss: 293372.2812\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 248037.4688 - val_loss: 341782.1250\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 241136.1406 - val_loss: 329142.5938\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 134067640.0000 - val_loss: 46461548.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 38707428.0000 - val_loss: 30363132.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 25020056.0000 - val_loss: 23809046.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 18061778.0000 - val_loss: 16728996.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 14877492.0000 - val_loss: 16658238.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 11947077.0000 - val_loss: 12120798.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9783981.0000 - val_loss: 9977335.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8731036.0000 - val_loss: 10140164.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7298742.5000 - val_loss: 8265109.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6155443.5000 - val_loss: 7270277.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5560350.0000 - val_loss: 6432842.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5172103.5000 - val_loss: 6125293.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4425359.0000 - val_loss: 6230829.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4504043.0000 - val_loss: 5236233.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4477134.0000 - val_loss: 7461161.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3811057.0000 - val_loss: 4412355.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3690481.7500 - val_loss: 4205843.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3348041.2500 - val_loss: 4108505.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2898397.2500 - val_loss: 3450953.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2620202.0000 - val_loss: 3527897.5000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3158021.0000 - val_loss: 4871798.5000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2429606.5000 - val_loss: 2901643.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2207198.0000 - val_loss: 2389828.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1787526.6250 - val_loss: 2680723.5000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1689268.5000 - val_loss: 1737674.5000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1550470.8750 - val_loss: 2152929.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1323031.0000 - val_loss: 1466215.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1624075.6250 - val_loss: 1709619.0000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1184513.6250 - val_loss: 1302376.3750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 903196.4375 - val_loss: 1392642.3750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 988418.2500 - val_loss: 1767523.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 629604.0000 - val_loss: 854127.6875\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 513652.0000 - val_loss: 622047.0000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 497642.1250 - val_loss: 580925.1250\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 607485.3750 - val_loss: 1268522.6250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 687178.8750 - val_loss: 790258.6250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 364363.0000 - val_loss: 505233.2812\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 414880.7812 - val_loss: 638361.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 355660.1250 - val_loss: 530787.5000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 267378.4688 - val_loss: 421722.9688\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 406335.8438 - val_loss: 934524.6875\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 306575.3438 - val_loss: 426134.0625\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 287853.6250 - val_loss: 690065.0000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 394822.3438 - val_loss: 627838.6875\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 284372.8750 - val_loss: 409850.1562\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 361113.5000 - val_loss: 685578.1875\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 251532.3906 - val_loss: 426200.1875\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 177803.6406 - val_loss: 413188.3438\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 232234.8125 - val_loss: 600449.0000\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 238437.6719 - val_loss: 361210.6562\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 239540.8281 - val_loss: 500606.1875\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 233397.1094 - val_loss: 408196.2188\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 190389.4688 - val_loss: 509332.6250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 230110.6406 - val_loss: 425698.6562\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 188457.4375 - val_loss: 576585.6250\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 243993.4219 - val_loss: 520018.7188\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 183533.2656 - val_loss: 356820.0000\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 308583.3750 - val_loss: 781090.5625\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 332110.8125 - val_loss: 635204.8125\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 164362.5156 - val_loss: 836389.8750\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 261267.7500 - val_loss: 388583.4375\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 184032.8281 - val_loss: 408356.3125\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 180200.7500 - val_loss: 327438.5938\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 387827.0625 - val_loss: 886189.1875\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 342359.5312 - val_loss: 527172.3750\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 247147.1719 - val_loss: 599632.0000\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 244569.6719 - val_loss: 358586.4062\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 142004.7344 - val_loss: 371993.7500\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 199431.2969 - val_loss: 631342.7500\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 227663.9688 - val_loss: 315572.5625\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 144061.3125 - val_loss: 331452.2812\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 139078.6719 - val_loss: 674135.6875\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 257982.0000 - val_loss: 454389.8750\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 219696.5781 - val_loss: 350904.3125\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 207634.3438 - val_loss: 331448.8125\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 147538.0000 - val_loss: 627416.2500\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 167672.4688 - val_loss: 425746.8750\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 391196.7500 - val_loss: 611661.2500\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 172578.3906 - val_loss: 882604.1875\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 275378.8750 - val_loss: 438300.1250\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 120757832.0000 - val_loss: 43398912.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 30731370.0000 - val_loss: 24072576.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 19930232.0000 - val_loss: 17448546.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 14540729.0000 - val_loss: 13679358.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11100590.0000 - val_loss: 9453395.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8561758.0000 - val_loss: 7979349.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7923961.0000 - val_loss: 7919990.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6852507.5000 - val_loss: 6985396.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5728798.0000 - val_loss: 5606483.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5787714.0000 - val_loss: 5743033.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5099774.0000 - val_loss: 4379184.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3819954.2500 - val_loss: 4728876.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3296966.5000 - val_loss: 3132431.2500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2960204.5000 - val_loss: 2694526.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2799698.0000 - val_loss: 3433167.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2648689.7500 - val_loss: 2588373.7500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2213740.7500 - val_loss: 2288517.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1943168.6250 - val_loss: 2595692.2500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1789368.7500 - val_loss: 2082057.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1582993.2500 - val_loss: 2000506.0000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1667424.5000 - val_loss: 2515901.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1429694.1250 - val_loss: 1792660.1250\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1401478.2500 - val_loss: 1496007.2500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1439007.5000 - val_loss: 1248628.5000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1416368.6250 - val_loss: 1206810.7500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1234304.6250 - val_loss: 1212244.8750\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1208143.3750 - val_loss: 1511795.6250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1020640.4375 - val_loss: 1298422.6250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1111778.6250 - val_loss: 1645178.7500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1052404.5000 - val_loss: 1079144.6250\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 965990.6875 - val_loss: 867872.8125\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 790865.7500 - val_loss: 781158.1875\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1019307.4375 - val_loss: 932628.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 805857.0625 - val_loss: 737709.1875\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 796306.4375 - val_loss: 690164.8125\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 664003.7500 - val_loss: 829374.2500\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 973074.5625 - val_loss: 1701082.2500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 818183.1875 - val_loss: 1397452.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 871632.2500 - val_loss: 1241164.8750\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 616153.6875 - val_loss: 669696.2500\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 580694.2500 - val_loss: 491308.6875\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 518621.0625 - val_loss: 513962.3750\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 636763.9375 - val_loss: 673664.2500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 496094.9375 - val_loss: 580942.0000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 669117.8750 - val_loss: 844124.0000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 501985.5625 - val_loss: 516112.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 531179.3750 - val_loss: 692029.9375\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 446695.1875 - val_loss: 491638.7812\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 422359.2812 - val_loss: 631679.0000\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 435093.9062 - val_loss: 611648.5625\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 489052.6250 - val_loss: 476709.2812\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 496862.2812 - val_loss: 844599.1250\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 467230.6562 - val_loss: 581126.3750\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 460158.6562 - val_loss: 614751.8125\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 426299.8750 - val_loss: 351528.9688\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 421311.0312 - val_loss: 489931.5312\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 414848.8125 - val_loss: 611920.1250\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 499602.6562 - val_loss: 414956.5625\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 384946.5938 - val_loss: 512238.3438\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 336599.7812 - val_loss: 347951.1562\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 483039.7812 - val_loss: 434422.6562\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 385871.8750 - val_loss: 526589.1250\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 456635.0625 - val_loss: 700892.7500\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 364761.3125 - val_loss: 461961.5000\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 334212.2188 - val_loss: 437350.5625\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 338219.3125 - val_loss: 512438.5000\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 355119.3750 - val_loss: 435013.8750\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 321338.6562 - val_loss: 323909.2188\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 378379.3125 - val_loss: 405865.7500\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 360418.6250 - val_loss: 849598.3750\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 329058.7188 - val_loss: 307622.7500\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 363825.7812 - val_loss: 430555.6250\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 398613.5312 - val_loss: 454276.1250\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 270063.8750 - val_loss: 438260.6562\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 359861.3750 - val_loss: 386996.5938\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 284684.2188 - val_loss: 297011.3125\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 370646.7500 - val_loss: 696529.1875\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 301796.5312 - val_loss: 207049.6562\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 258226.2500 - val_loss: 236970.8125\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 239031.9375 - val_loss: 300501.0312\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 236688.5781 - val_loss: 535741.6250\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 264974.6875 - val_loss: 326628.1875\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 228526.4062 - val_loss: 226422.8281\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 214669.3906 - val_loss: 359850.7188\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 241410.5781 - val_loss: 194192.0312\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 222235.0781 - val_loss: 290866.2812\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 178899.5781 - val_loss: 379367.8750\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 480793.1250 - val_loss: 272149.0625\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 318622.8125 - val_loss: 255176.8594\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 163371.3594 - val_loss: 148441.3438\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 175952.5000 - val_loss: 163517.4219\n",
      "Epoch 92/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 262725.8750 - val_loss: 156643.4375\n",
      "Epoch 93/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 179946.0625 - val_loss: 170376.3281\n",
      "Epoch 94/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 172522.2969 - val_loss: 147223.3594\n",
      "Epoch 95/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 161592.0312 - val_loss: 300981.2812\n",
      "Epoch 96/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 163261.5625 - val_loss: 320736.9062\n",
      "Epoch 97/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 167849.1875 - val_loss: 240958.0469\n",
      "Epoch 98/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 238564.3281 - val_loss: 288527.8750\n",
      "Epoch 99/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 198246.1875 - val_loss: 257242.4844\n",
      "Epoch 100/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 144018.9219 - val_loss: 143968.7656\n",
      "Epoch 101/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 138297.7031 - val_loss: 194186.5625\n",
      "Epoch 102/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 160549.5156 - val_loss: 186593.6562\n",
      "Epoch 103/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 138411.8125 - val_loss: 291996.6875\n",
      "Epoch 104/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 201421.2188 - val_loss: 298829.0625\n",
      "Epoch 105/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 212056.8125 - val_loss: 297907.8750\n",
      "Epoch 106/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 238551.9375 - val_loss: 63134.7109\n",
      "Epoch 107/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 160297.6875 - val_loss: 186768.4844\n",
      "Epoch 108/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 157193.5312 - val_loss: 122548.2734\n",
      "Epoch 109/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 165144.4531 - val_loss: 98464.1641\n",
      "Epoch 110/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 231633.1875 - val_loss: 166092.1094\n",
      "Epoch 111/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 170569.3594 - val_loss: 173772.7656\n",
      "Epoch 112/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 172325.9375 - val_loss: 120947.2812\n",
      "Epoch 113/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 182520.0156 - val_loss: 315772.6562\n",
      "Epoch 114/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 156746.1875 - val_loss: 208312.5312\n",
      "Epoch 115/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 153408.2031 - val_loss: 149505.7031\n",
      "Epoch 116/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 169792.5625 - val_loss: 387773.9375\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 147.8266 - val_loss: 136.7262\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.0240 - val_loss: 136.7949\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.7275 - val_loss: 134.3967\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.6526 - val_loss: 136.1326\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.9900 - val_loss: 132.9092\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.8270 - val_loss: 131.5531\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.4650 - val_loss: 132.6440\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3012 - val_loss: 131.7534\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.9218 - val_loss: 130.8399\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.0704 - val_loss: 131.4545\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.3156 - val_loss: 130.3195\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.4937 - val_loss: 130.9870\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.5859 - val_loss: 130.3738\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.6754 - val_loss: 129.6770\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.9009 - val_loss: 130.6807\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.8632 - val_loss: 129.9926\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.7503 - val_loss: 129.1325\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.3931 - val_loss: 131.0025\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.2353 - val_loss: 132.6269\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.4644 - val_loss: 130.7410\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.4746 - val_loss: 130.0461\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.9839 - val_loss: 128.0609\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.9068 - val_loss: 129.0089\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.2159 - val_loss: 130.7664\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5835 - val_loss: 128.8950\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.1447 - val_loss: 130.1801\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.7449 - val_loss: 129.4750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.7649 - val_loss: 128.5044\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7025 - val_loss: 129.5719\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.6866 - val_loss: 128.1634\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.4283 - val_loss: 129.3896\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.4951 - val_loss: 128.9800\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 10ms/step - loss: 161.5240 - val_loss: 153.5739\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 140.3818 - val_loss: 137.8710\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 137.3153 - val_loss: 139.6696\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 137.4921 - val_loss: 136.7712\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 137.2548 - val_loss: 137.8735\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 133.1026 - val_loss: 134.4478\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.6920 - val_loss: 136.7288\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.4687 - val_loss: 133.6396\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.3995 - val_loss: 135.7032\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.9521 - val_loss: 131.3166\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.8552 - val_loss: 132.9641\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.4323 - val_loss: 131.3807\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.4781 - val_loss: 130.6465\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.1047 - val_loss: 131.4891\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.3805 - val_loss: 129.3942\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6369 - val_loss: 131.6132\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.0655 - val_loss: 129.6557\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9999 - val_loss: 130.2243\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.4651 - val_loss: 131.7323\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.0317 - val_loss: 129.8575\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.6038 - val_loss: 129.2018\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.0304 - val_loss: 130.8569\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.4633 - val_loss: 128.2816\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.3472 - val_loss: 129.8869\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.3287 - val_loss: 128.5601\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6322 - val_loss: 130.2115\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9219 - val_loss: 129.6424\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.7336 - val_loss: 132.3549\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.8135 - val_loss: 129.2425\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4954 - val_loss: 128.8316\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.8315 - val_loss: 128.0480\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.1951 - val_loss: 129.9713\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4748 - val_loss: 128.4156\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4295 - val_loss: 128.7627\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6225 - val_loss: 129.8456\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.4577 - val_loss: 129.4961\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4159 - val_loss: 129.0765\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3285 - val_loss: 128.4860\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.5847 - val_loss: 129.5117\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5654 - val_loss: 129.8808\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6345 - val_loss: 131.3253\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 142.8492 - val_loss: 142.3335\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.6411 - val_loss: 136.3103\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.7396 - val_loss: 133.9076\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.0671 - val_loss: 131.4047\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.3795 - val_loss: 131.3641\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.5838 - val_loss: 133.0601\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.5070 - val_loss: 132.0494\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.9623 - val_loss: 133.1762\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.3397 - val_loss: 132.1483\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.8154 - val_loss: 131.0271\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3541 - val_loss: 131.6383\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.6897 - val_loss: 131.1051\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3285 - val_loss: 133.9290\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.1999 - val_loss: 134.6884\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.3581 - val_loss: 130.8957\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.7836 - val_loss: 131.7789\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7041 - val_loss: 131.3132\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.0075 - val_loss: 132.5450\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.0989 - val_loss: 131.4504\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6916 - val_loss: 132.4530\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.0363 - val_loss: 129.3936\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0123 - val_loss: 129.3636\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0415 - val_loss: 130.7996\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.0282 - val_loss: 129.3984\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5012 - val_loss: 129.3909\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3236 - val_loss: 128.9689\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6160 - val_loss: 128.3758\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.9771 - val_loss: 130.4169\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3116 - val_loss: 129.2051\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.0825 - val_loss: 129.4041\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.2124 - val_loss: 129.9733\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.1233 - val_loss: 129.8245\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.1870 - val_loss: 128.9268\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.7789 - val_loss: 129.1173\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.1501 - val_loss: 129.5347\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.6274 - val_loss: 128.3719\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3192 - val_loss: 128.6763\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.0600 - val_loss: 131.9155\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.0770 - val_loss: 128.5991\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.4978 - val_loss: 127.8734\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.0106 - val_loss: 128.9290\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8839 - val_loss: 128.6314\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.7695 - val_loss: 127.7689\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.0977 - val_loss: 127.9332\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.8651 - val_loss: 127.7407\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.5136 - val_loss: 127.3005\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.9697 - val_loss: 128.3365\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.5654 - val_loss: 127.9941\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7574 - val_loss: 129.2689\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7260 - val_loss: 128.6023\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.1696 - val_loss: 127.5562\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.9363 - val_loss: 127.1224\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.0425 - val_loss: 130.3250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.9427 - val_loss: 128.5994\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.4365 - val_loss: 127.0019\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.9486 - val_loss: 127.4167\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.1732 - val_loss: 129.0146\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.9745 - val_loss: 129.2045\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.8526 - val_loss: 128.3843\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.5668 - val_loss: 127.8036\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.9931 - val_loss: 128.4084\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.0560 - val_loss: 127.6209\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.5256 - val_loss: 129.2761\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.5735 - val_loss: 127.7553\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.0853 - val_loss: 128.9752\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 137.3411 - val_loss: 137.6224\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 134.0478 - val_loss: 141.0311\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.2041 - val_loss: 133.6349\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.6179 - val_loss: 132.2969\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.6609 - val_loss: 132.6642\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.1158 - val_loss: 133.2966\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.3318 - val_loss: 131.5846\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.2489 - val_loss: 132.3594\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.2281 - val_loss: 135.6625\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 130.8749 - val_loss: 132.7053\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.1748 - val_loss: 132.4237\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.7720 - val_loss: 130.6287\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.4121 - val_loss: 133.0193\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 126.9483 - val_loss: 130.5037\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.3223 - val_loss: 131.1031\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.9297 - val_loss: 131.1313\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2594 - val_loss: 131.7218\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5040 - val_loss: 129.7085\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.2939 - val_loss: 130.4133\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6762 - val_loss: 129.8521\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.5504 - val_loss: 129.5795\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.5431 - val_loss: 131.1724\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.6096 - val_loss: 130.9598\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7575 - val_loss: 132.8971\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.8713 - val_loss: 130.6032\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.0942 - val_loss: 130.7041\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.8910 - val_loss: 129.6274\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.8645 - val_loss: 130.5415\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.4254 - val_loss: 128.9043\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6117 - val_loss: 129.9761\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.0484 - val_loss: 129.6614\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.2334 - val_loss: 131.3465\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.5565 - val_loss: 128.2209\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4698 - val_loss: 129.7401\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3254 - val_loss: 130.7755\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9540 - val_loss: 131.8996\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9478 - val_loss: 129.6784\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.1112 - val_loss: 129.1518\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7109 - val_loss: 128.8847\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6962 - val_loss: 129.5756\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.7553 - val_loss: 130.1289\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6884 - val_loss: 128.0447\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.7974 - val_loss: 129.3438\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.7099 - val_loss: 128.5058\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.1652 - val_loss: 128.3576\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6825 - val_loss: 129.2152\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0982 - val_loss: 129.7170\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9051 - val_loss: 129.0688\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.8726 - val_loss: 129.0228\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.2689 - val_loss: 128.4655\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8006 - val_loss: 130.1082\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.0501 - val_loss: 128.7373\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 144.3060 - val_loss: 136.0902\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.9797 - val_loss: 134.2024\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.2713 - val_loss: 134.2591\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.2840 - val_loss: 132.2233\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.7891 - val_loss: 131.1339\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3672 - val_loss: 134.6154\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5898 - val_loss: 132.0708\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.8483 - val_loss: 130.8167\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.7929 - val_loss: 130.3080\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.6891 - val_loss: 133.2978\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.0517 - val_loss: 132.1542\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.4727 - val_loss: 133.7507\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.2485 - val_loss: 132.5942\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.0544 - val_loss: 130.9396\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.4562 - val_loss: 129.5341\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.8695 - val_loss: 130.0792\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.5178 - val_loss: 128.9208\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.5058 - val_loss: 131.5927\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.2390 - val_loss: 130.5087\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.3853 - val_loss: 130.1805\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.8067 - val_loss: 129.3699\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6482 - val_loss: 128.4610\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7523 - val_loss: 129.4723\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9745 - val_loss: 130.6015\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.4383 - val_loss: 129.6986\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.3582 - val_loss: 132.6803\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.9605 - val_loss: 130.2162\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.0688 - val_loss: 129.6141\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.1525 - val_loss: 128.7022\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6802 - val_loss: 132.0947\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.9369 - val_loss: 129.0875\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.3044 - val_loss: 129.4498\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 143.2365 - val_loss: 139.2883\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.8321 - val_loss: 134.8463\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.9863 - val_loss: 135.2438\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.9153 - val_loss: 133.6225\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.4136 - val_loss: 131.4677\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.4021 - val_loss: 132.4350\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.6368 - val_loss: 129.7486\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.2011 - val_loss: 131.6459\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.1892 - val_loss: 130.7739\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.5788 - val_loss: 131.8578\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7571 - val_loss: 131.2004\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.3644 - val_loss: 132.1176\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5612 - val_loss: 133.6805\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.4306 - val_loss: 131.4029\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.0854 - val_loss: 130.5876\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.0769 - val_loss: 131.1575\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4855 - val_loss: 129.4540\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.5862 - val_loss: 129.4239\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.4104 - val_loss: 129.8667\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9323 - val_loss: 131.9596\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6906 - val_loss: 131.3830\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6477 - val_loss: 129.9362\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.1961 - val_loss: 131.7776\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8140 - val_loss: 131.5143\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7027 - val_loss: 131.7552\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.3900 - val_loss: 128.4284\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.6731 - val_loss: 129.3727\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.0964 - val_loss: 129.0805\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.8512 - val_loss: 128.5921\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4223 - val_loss: 129.9357\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7083 - val_loss: 129.2559\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3390 - val_loss: 129.5455\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4587 - val_loss: 129.9250\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3074 - val_loss: 128.5222\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.9174 - val_loss: 128.5666\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.5581 - val_loss: 128.9091\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 3s 8ms/step - loss: 139.2724 - val_loss: 138.4346\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.5930 - val_loss: 136.2719\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 131.8239 - val_loss: 136.3623\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 134.1355 - val_loss: 140.7162\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.0287 - val_loss: 132.5847\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.1560 - val_loss: 135.2088\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.3786 - val_loss: 132.9256\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.7441 - val_loss: 133.7527\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.8262 - val_loss: 132.7586\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.5717 - val_loss: 133.4583\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.3417 - val_loss: 130.4869\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.2280 - val_loss: 133.2848\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.0500 - val_loss: 131.9691\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.8678 - val_loss: 130.0927\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.2802 - val_loss: 133.9540\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.1029 - val_loss: 130.6748\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.0004 - val_loss: 129.6045\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.6234 - val_loss: 129.5542\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.9893 - val_loss: 130.5013\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.9065 - val_loss: 130.5135\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.1923 - val_loss: 129.6484\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.1154 - val_loss: 133.2512\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.2379 - val_loss: 131.3276\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.2392 - val_loss: 130.4743\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.4695 - val_loss: 128.8258\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.9579 - val_loss: 128.6017\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.0407 - val_loss: 129.9646\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.8683 - val_loss: 129.5669\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.4334 - val_loss: 128.8504\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5822 - val_loss: 128.5210\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6267 - val_loss: 128.5534\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5158 - val_loss: 129.7201\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7310 - val_loss: 129.5669\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.5191 - val_loss: 129.8448\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.0719 - val_loss: 130.8936\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4353 - val_loss: 128.3491\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.6340 - val_loss: 129.9896\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.7406 - val_loss: 128.2178\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.3933 - val_loss: 128.3697\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.5530 - val_loss: 129.9662\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.2146 - val_loss: 129.2037\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0853 - val_loss: 130.6351\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3877 - val_loss: 129.0808\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.6975 - val_loss: 129.2888\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.3887 - val_loss: 131.6445\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.8959 - val_loss: 129.6828\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.6055 - val_loss: 129.4261\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.4139 - val_loss: 129.0568\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 139.3653 - val_loss: 135.7872\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.7515 - val_loss: 135.2645\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.3791 - val_loss: 133.3920\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.0691 - val_loss: 131.7553\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 129.5090 - val_loss: 132.4851\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.0072 - val_loss: 135.4556\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.0466 - val_loss: 130.7620\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.1798 - val_loss: 129.8403\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.4881 - val_loss: 135.9084\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3781 - val_loss: 131.4271\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.4269 - val_loss: 130.0936\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.2909 - val_loss: 131.2558\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.3549 - val_loss: 130.6436\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.3792 - val_loss: 129.7292\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.2544 - val_loss: 135.5211\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9735 - val_loss: 130.5244\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9975 - val_loss: 130.3508\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.8172 - val_loss: 129.4960\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.2746 - val_loss: 129.0578\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.5585 - val_loss: 131.1565\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.0686 - val_loss: 130.6228\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9167 - val_loss: 128.2723\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5583 - val_loss: 128.9245\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.1511 - val_loss: 128.6164\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.3656 - val_loss: 131.5266\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.2546 - val_loss: 128.3103\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3660 - val_loss: 134.3005\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.8635 - val_loss: 130.5816\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.5109 - val_loss: 129.9948\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.3506 - val_loss: 129.5907\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.1229 - val_loss: 128.0915\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3444 - val_loss: 129.7626\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.2402 - val_loss: 130.7862\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.7426 - val_loss: 129.3555\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4738 - val_loss: 128.8952\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3816 - val_loss: 129.8036\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.5277 - val_loss: 129.8582\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.1852 - val_loss: 129.7403\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.0383 - val_loss: 127.2113\n",
      "Epoch 40/300\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 122.0357"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),300,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',300,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),300,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c961ec8-129c-4f36-a2dc-04b0b9661a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, test_X)\n",
    "mase_predictions = bagging_predict2(pred2, test_X)\n",
    "mape_predictions = bagging_predict2(pred3, test_X)\n",
    "concat_I = np.concatenate([smape_predictions, mase_predictions,mape_predictions],axis=0)\n",
    "fin_pred_I = np.median(concat_I,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "mean_squared_error(test_y.flatten(),fin_pred_I.flatten()),mean_absolute_error(test_y.flatten(),fin_pred_I.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02005ee4-9b13-4a5a-a2c3-d25441395630",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fin_pred_I.reshape(-1,24)).to_csv(\"../result7_new/NBEATs_B/pred_mid_I.csv\")\n",
    "for i in range(10):\n",
    "    pd.DataFrame(concat_I[i].reshape(-1,24)).to_csv(f\"../result7_new/NBEATs_B/pred_I{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a104b3-22fb-46e4-855e-ca5e5202a204",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 일반블락"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7fa618-a25e-48c5-9544-ca3e091f28ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 15:42:16.431436: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-09-04 15:42:16.431513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-09-04 15:42:16.432104: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.1650 - val_loss: 0.7374\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8452 - val_loss: 0.7208\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8042 - val_loss: 0.6587\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7835 - val_loss: 0.6515\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7655 - val_loss: 0.6321\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7569 - val_loss: 0.6739\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7490 - val_loss: 0.6326\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7309 - val_loss: 0.6158\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7461 - val_loss: 0.6315\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7146 - val_loss: 0.6118\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7159 - val_loss: 0.6289\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7079 - val_loss: 0.6103\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7004 - val_loss: 0.6111\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6822 - val_loss: 0.6132\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6891 - val_loss: 0.6410\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6938 - val_loss: 0.6277\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6725 - val_loss: 0.6528\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6797 - val_loss: 0.6208\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.6332\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.6389\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6386 - val_loss: 0.6634\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6253 - val_loss: 0.6327\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.1079 - val_loss: 0.7087\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8373 - val_loss: 0.6597\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7925 - val_loss: 0.6503\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7801 - val_loss: 0.6503\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7577 - val_loss: 0.6599\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7709 - val_loss: 0.6280\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7417 - val_loss: 0.6141\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7279 - val_loss: 0.6198\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7190 - val_loss: 0.6140\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7127 - val_loss: 0.6072\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7133 - val_loss: 0.6059\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6995 - val_loss: 0.6042\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6869 - val_loss: 0.6047\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6943 - val_loss: 0.6413\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6741 - val_loss: 0.6270\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6659 - val_loss: 0.6154\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6695 - val_loss: 0.6277\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.6139\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.6263\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6310 - val_loss: 0.6107\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6219 - val_loss: 0.6493\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6217 - val_loss: 0.6546\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.0651 - val_loss: 0.7289\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8387 - val_loss: 0.6797\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7956 - val_loss: 0.6537\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7818 - val_loss: 0.6936\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7725 - val_loss: 0.6484\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7447 - val_loss: 0.6274\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7505 - val_loss: 0.6279\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7276 - val_loss: 0.6160\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7241 - val_loss: 0.6413\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7253 - val_loss: 0.6284\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7199 - val_loss: 0.6072\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7126 - val_loss: 0.6370\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7080 - val_loss: 0.6175\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6891 - val_loss: 0.6130\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6824 - val_loss: 0.6249\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6850 - val_loss: 0.5994\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6684 - val_loss: 0.6318\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6514 - val_loss: 0.6165\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.6300\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6456 - val_loss: 0.6208\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.6029\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6252 - val_loss: 0.6083\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6210 - val_loss: 0.6051\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5916 - val_loss: 0.6221\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.6242\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5959 - val_loss: 0.6149\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.0572 - val_loss: 0.7241\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8406 - val_loss: 0.6835\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7927 - val_loss: 0.6582\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7772 - val_loss: 0.6447\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7671 - val_loss: 0.6290\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7453 - val_loss: 0.6415\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7339 - val_loss: 0.6236\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7294 - val_loss: 0.6298\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7158 - val_loss: 0.6051\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7090 - val_loss: 0.6117\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6950 - val_loss: 0.6247\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7086 - val_loss: 0.6059\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6760 - val_loss: 0.6046\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7010 - val_loss: 0.6028\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6804 - val_loss: 0.6177\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6731 - val_loss: 0.6167\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6566 - val_loss: 0.6019\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6570 - val_loss: 0.6054\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6408 - val_loss: 0.6549\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 0.6206\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6361 - val_loss: 0.6019\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6192 - val_loss: 0.6431\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6326 - val_loss: 0.6261\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6014 - val_loss: 0.6364\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.6181\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5728 - val_loss: 0.6221\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5702 - val_loss: 0.6540\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5662 - val_loss: 0.6346\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5514 - val_loss: 0.6329\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 0.6816\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5394 - val_loss: 0.6408\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.0506 - val_loss: 0.7127\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8281 - val_loss: 0.6689\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7980 - val_loss: 0.6670\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7781 - val_loss: 0.6467\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7540 - val_loss: 0.6256\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7468 - val_loss: 0.6313\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7382 - val_loss: 0.6892\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7368 - val_loss: 0.6218\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7201 - val_loss: 0.6478\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7117 - val_loss: 0.6264\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6947 - val_loss: 0.6026\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6972 - val_loss: 0.6411\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6814 - val_loss: 0.6212\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6780 - val_loss: 0.6296\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6551 - val_loss: 0.6372\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6618 - val_loss: 0.6189\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6632 - val_loss: 0.6109\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6349 - val_loss: 0.6170\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.6265\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6221 - val_loss: 0.6336\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5897 - val_loss: 0.6070\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.1118 - val_loss: 0.7271\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8491 - val_loss: 0.6684\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8023 - val_loss: 0.6720\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7731 - val_loss: 0.6664\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7681 - val_loss: 0.6530\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7475 - val_loss: 0.6225\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7518 - val_loss: 0.6362\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7304 - val_loss: 0.6175\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7170 - val_loss: 0.6164\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7171 - val_loss: 0.6232\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7028 - val_loss: 0.6442\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7168 - val_loss: 0.6306\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6913 - val_loss: 0.6188\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6823 - val_loss: 0.6077\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6975 - val_loss: 0.6550\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6718 - val_loss: 0.6372\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 0.6124\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6463 - val_loss: 0.6438\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6369 - val_loss: 0.6115\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6293 - val_loss: 0.6141\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6153 - val_loss: 0.6197\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 0.6260\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 0.6113\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 0.6177\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.0976 - val_loss: 0.7669\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8344 - val_loss: 0.6654\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8046 - val_loss: 0.6674\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7805 - val_loss: 0.6438\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7631 - val_loss: 0.6485\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7614 - val_loss: 0.6523\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7410 - val_loss: 0.6340\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7342 - val_loss: 0.6421\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7338 - val_loss: 0.6370\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7157 - val_loss: 0.6570\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7028 - val_loss: 0.6066\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7021 - val_loss: 0.6292\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6911 - val_loss: 0.6196\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6851 - val_loss: 0.6166\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6898 - val_loss: 0.6136\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6953 - val_loss: 0.6109\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6855 - val_loss: 0.6230\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6583 - val_loss: 0.6092\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7041 - val_loss: 0.6290\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6406 - val_loss: 0.6303\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6236 - val_loss: 0.6061\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 0.6202\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6175 - val_loss: 0.6297\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6027 - val_loss: 0.6361\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.6218\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6042 - val_loss: 0.6378\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5640 - val_loss: 0.6139\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5483 - val_loss: 0.6898\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5711 - val_loss: 0.6190\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5476 - val_loss: 0.6404\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5443 - val_loss: 0.6762\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.1008 - val_loss: 0.7048\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8258 - val_loss: 0.6505\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7965 - val_loss: 0.6637\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7843 - val_loss: 0.6991\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7606 - val_loss: 0.6309\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7506 - val_loss: 0.6200\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7409 - val_loss: 0.6283\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7283 - val_loss: 0.6077\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7145 - val_loss: 0.6118\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7113 - val_loss: 0.6200\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7260 - val_loss: 0.6078\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7039 - val_loss: 0.5929\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6882 - val_loss: 0.6133\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6755 - val_loss: 0.6143\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6760 - val_loss: 0.6014\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6746 - val_loss: 0.6091\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6512 - val_loss: 0.6199\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6539 - val_loss: 0.5985\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6393 - val_loss: 0.6076\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6339 - val_loss: 0.6195\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6313 - val_loss: 0.5918\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6075 - val_loss: 0.6165\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5964 - val_loss: 0.6186\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.6168\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5916 - val_loss: 0.6211\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5564 - val_loss: 0.6358\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5592 - val_loss: 0.5934\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5614 - val_loss: 0.6454\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5638 - val_loss: 0.5924\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5385 - val_loss: 0.5924\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5307 - val_loss: 0.6037\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.0994 - val_loss: 0.7278\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8294 - val_loss: 0.6761\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7794 - val_loss: 0.6666\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7648 - val_loss: 0.6544\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7538 - val_loss: 0.6340\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7479 - val_loss: 0.6401\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7319 - val_loss: 0.6241\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7190 - val_loss: 0.6135\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7147 - val_loss: 0.6384\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7090 - val_loss: 0.6188\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7094 - val_loss: 0.6239\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7027 - val_loss: 0.6244\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6856 - val_loss: 0.6123\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6789 - val_loss: 0.6087\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6807 - val_loss: 0.6475\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.6311\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6619 - val_loss: 0.6037\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6336 - val_loss: 0.6304\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.6338\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6439 - val_loss: 0.6403\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 0.6187\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6104 - val_loss: 0.6232\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6096 - val_loss: 0.6257\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 0.6174\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.6088\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5678 - val_loss: 0.6449\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5625 - val_loss: 0.6358\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 1.1176 - val_loss: 0.7258\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8366 - val_loss: 0.6595\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7927 - val_loss: 0.6540\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7791 - val_loss: 0.6809\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7609 - val_loss: 0.6384\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7516 - val_loss: 0.6649\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7414 - val_loss: 0.6369\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7345 - val_loss: 0.6060\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7200 - val_loss: 0.6098\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7126 - val_loss: 0.6200\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7102 - val_loss: 0.6274\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7031 - val_loss: 0.6222\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6949 - val_loss: 0.6122\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7027 - val_loss: 0.6050\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6755 - val_loss: 0.6062\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6746 - val_loss: 0.6141\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6697 - val_loss: 0.6784\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6641 - val_loss: 0.6055\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.6081\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.6932\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.6182\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6441 - val_loss: 0.6171\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6209 - val_loss: 0.6011\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.6041\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.6638\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 0.6166\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5969 - val_loss: 0.6236\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5701 - val_loss: 0.6166\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5722 - val_loss: 0.6062\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5500 - val_loss: 0.6494\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 0.6530\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5378 - val_loss: 0.6639\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5186 - val_loss: 0.6550\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 50026328.0000 - val_loss: 24553344.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 19240204.0000 - val_loss: 16315291.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 13000347.0000 - val_loss: 11338526.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10282098.0000 - val_loss: 11231007.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7988108.5000 - val_loss: 7672589.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6251965.5000 - val_loss: 6727541.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5146118.0000 - val_loss: 5346918.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4564961.0000 - val_loss: 5147508.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3819102.2500 - val_loss: 4024162.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3233240.5000 - val_loss: 3873576.2500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2871531.7500 - val_loss: 3384035.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2474402.2500 - val_loss: 3753644.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2425637.2500 - val_loss: 2981995.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2023732.5000 - val_loss: 2386329.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2023698.6250 - val_loss: 2712668.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1783473.6250 - val_loss: 2642840.2500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1633911.6250 - val_loss: 2222778.7500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1423045.6250 - val_loss: 2021776.1250\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1410205.7500 - val_loss: 1879927.8750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1326206.1250 - val_loss: 1878583.8750\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1136785.8750 - val_loss: 1647997.1250\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1158698.7500 - val_loss: 1295024.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1060938.3750 - val_loss: 1564518.1250\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1099779.5000 - val_loss: 1861042.5000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 953284.0000 - val_loss: 1294778.0000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 832844.8750 - val_loss: 1168672.7500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 827490.2500 - val_loss: 1474589.2500\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 876844.0000 - val_loss: 1359266.1250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 880179.6875 - val_loss: 1522144.7500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 870474.8125 - val_loss: 1049839.0000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 826781.6875 - val_loss: 1373637.1250\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 917513.1875 - val_loss: 1568229.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 867014.6875 - val_loss: 1248569.0000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 790003.9375 - val_loss: 1408849.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 774123.4375 - val_loss: 1047999.2500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 799514.0000 - val_loss: 1305583.3750\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 713060.2500 - val_loss: 1047313.6250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 679168.2500 - val_loss: 1057877.3750\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 655551.2500 - val_loss: 1077771.6250\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 674991.9375 - val_loss: 1370490.6250\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 685135.2500 - val_loss: 1019140.5000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 622490.9375 - val_loss: 1490035.5000\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 706227.3750 - val_loss: 1265446.5000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 652184.0000 - val_loss: 1128556.2500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 639027.2500 - val_loss: 1263996.3750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 636472.9375 - val_loss: 1265605.7500\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 654567.6250 - val_loss: 830168.9375\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 618336.5000 - val_loss: 1342589.6250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 622320.0000 - val_loss: 943897.8125\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 550063.7500 - val_loss: 1132100.6250\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 627767.3125 - val_loss: 988041.1875\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 622617.8750 - val_loss: 1324943.0000\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 731663.9375 - val_loss: 1036435.5000\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 650590.5000 - val_loss: 1520461.7500\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 601816.3750 - val_loss: 1136485.3750\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 710622.8125 - val_loss: 1125258.8750\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 599155.9375 - val_loss: 1240701.6250\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 44840564.0000 - val_loss: 23674558.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 17649406.0000 - val_loss: 14215661.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11326935.0000 - val_loss: 10836288.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8957581.0000 - val_loss: 7625589.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7124822.0000 - val_loss: 7101582.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5729334.5000 - val_loss: 6280597.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5109770.5000 - val_loss: 5669748.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4348726.0000 - val_loss: 4726216.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3860049.7500 - val_loss: 4320659.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3223104.7500 - val_loss: 3917660.2500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2992797.5000 - val_loss: 4379307.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2740767.5000 - val_loss: 3430962.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2320496.0000 - val_loss: 3333438.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2230785.7500 - val_loss: 3180827.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2176073.5000 - val_loss: 2951295.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2123125.2500 - val_loss: 3828132.2500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2009224.0000 - val_loss: 2168095.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1670707.2500 - val_loss: 2213762.7500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1456866.7500 - val_loss: 2241860.7500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1495206.6250 - val_loss: 2094604.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1265147.2500 - val_loss: 2004455.5000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1349134.0000 - val_loss: 2072692.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1246448.0000 - val_loss: 1744257.3750\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1262237.7500 - val_loss: 2144951.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1129380.8750 - val_loss: 1904652.1250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1087108.2500 - val_loss: 1622463.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 961435.2500 - val_loss: 1462032.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1146805.7500 - val_loss: 1876605.6250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1152431.6250 - val_loss: 1616938.6250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 912678.8125 - val_loss: 1459390.3750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 972077.0000 - val_loss: 1536546.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 946117.1250 - val_loss: 1494281.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 922399.3750 - val_loss: 1391004.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 916660.8125 - val_loss: 1516099.2500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 851640.6250 - val_loss: 1670844.7500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 812000.0625 - val_loss: 1500455.0000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 820732.3125 - val_loss: 1319519.7500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 663001.7500 - val_loss: 1077537.1250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 784254.6250 - val_loss: 1397535.2500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 720554.1875 - val_loss: 1359562.8750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 819957.0000 - val_loss: 1308302.3750\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 677430.6875 - val_loss: 1385437.6250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 680688.0000 - val_loss: 1287146.1250\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 670344.0000 - val_loss: 1077513.2500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 765475.2500 - val_loss: 1283339.7500\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 685875.3750 - val_loss: 1036581.0625\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 599590.3750 - val_loss: 1063627.7500\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 619362.2500 - val_loss: 1228264.2500\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 693642.1250 - val_loss: 1002622.8750\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 637734.0625 - val_loss: 1050147.2500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 674787.3125 - val_loss: 1078623.2500\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 625570.7500 - val_loss: 1562985.8750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 646682.3750 - val_loss: 1124879.5000\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 615743.3750 - val_loss: 1055253.1250\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 714973.0000 - val_loss: 1281086.5000\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 571077.6250 - val_loss: 1081361.2500\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 606794.8750 - val_loss: 1286308.6250\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 554005.7500 - val_loss: 1131635.7500\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 768394.5000 - val_loss: 1295908.0000\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 45072472.0000 - val_loss: 21316690.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 16765468.0000 - val_loss: 15186831.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10559244.0000 - val_loss: 9245226.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7871544.0000 - val_loss: 7770035.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6439558.0000 - val_loss: 7907209.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5230460.5000 - val_loss: 6089042.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4421454.0000 - val_loss: 5227357.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3668019.0000 - val_loss: 4850675.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3232828.7500 - val_loss: 3254414.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2608653.0000 - val_loss: 3631979.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2450373.2500 - val_loss: 3075136.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2259202.2500 - val_loss: 2749244.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1886034.2500 - val_loss: 2777335.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1660678.6250 - val_loss: 1959693.2500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1585268.5000 - val_loss: 2204547.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1509153.7500 - val_loss: 1856754.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1472261.3750 - val_loss: 2144227.2500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1346291.3750 - val_loss: 1928863.7500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1158591.5000 - val_loss: 1986541.8750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1178892.0000 - val_loss: 1574175.0000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1067595.6250 - val_loss: 1406401.1250\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 962660.1250 - val_loss: 1436277.6250\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 930806.8125 - val_loss: 1737129.8750\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 928352.6875 - val_loss: 1749817.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 972114.1250 - val_loss: 1329038.1250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 790735.6250 - val_loss: 1384997.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 807012.8125 - val_loss: 1459458.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 809922.6250 - val_loss: 1149272.1250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 786613.7500 - val_loss: 1091227.7500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 808018.9375 - val_loss: 1125342.0000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 701717.7500 - val_loss: 1003221.4375\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 723038.4375 - val_loss: 1319808.0000\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 762393.5000 - val_loss: 1138214.1250\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 588489.0000 - val_loss: 1237093.5000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 647412.1250 - val_loss: 999265.1250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 573931.7500 - val_loss: 1110350.6250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 658517.8125 - val_loss: 903497.0625\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 650060.6875 - val_loss: 1283708.8750\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 671392.8750 - val_loss: 1075342.8750\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 678586.9375 - val_loss: 970845.5625\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 677044.9375 - val_loss: 1251625.0000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 714082.1250 - val_loss: 1384410.0000\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 618786.6250 - val_loss: 913205.6250\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 666010.2500 - val_loss: 1066518.5000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 650717.5000 - val_loss: 1023479.6250\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 590551.3125 - val_loss: 1125512.8750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 590977.2500 - val_loss: 992966.9375\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 53920124.0000 - val_loss: 22668074.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 20184876.0000 - val_loss: 19029444.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 13282283.0000 - val_loss: 12056828.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10412942.0000 - val_loss: 10554729.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8301340.5000 - val_loss: 8658001.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6631012.5000 - val_loss: 8759159.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6067268.0000 - val_loss: 6555429.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5098408.0000 - val_loss: 6474725.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4567643.0000 - val_loss: 5194850.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3906574.7500 - val_loss: 4210311.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3493398.0000 - val_loss: 4984393.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3304453.7500 - val_loss: 4095640.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3179671.2500 - val_loss: 3315453.2500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2672208.5000 - val_loss: 4073280.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2506824.7500 - val_loss: 2946266.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2240459.7500 - val_loss: 3332580.7500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2319974.0000 - val_loss: 2482353.7500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1810944.8750 - val_loss: 2218717.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1831193.1250 - val_loss: 2422226.7500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1845903.3750 - val_loss: 2478654.5000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1612341.5000 - val_loss: 2258608.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1600142.5000 - val_loss: 2638297.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1604146.8750 - val_loss: 1684297.5000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1444655.2500 - val_loss: 1968265.0000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1288995.2500 - val_loss: 1964106.6250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1291836.1250 - val_loss: 2073029.1250\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1264550.1250 - val_loss: 1907270.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1077969.5000 - val_loss: 1496731.6250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1042503.4375 - val_loss: 1411747.2500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1011918.6875 - val_loss: 1696544.2500\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1060617.2500 - val_loss: 1366671.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 982592.2500 - val_loss: 1573521.6250\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 911067.7500 - val_loss: 1385599.1250\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 844206.1875 - val_loss: 1363231.8750\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 912079.7500 - val_loss: 1251719.5000\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 851139.2500 - val_loss: 1491390.0000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 808625.4375 - val_loss: 1319473.2500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 786148.7500 - val_loss: 1291701.1250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 785477.4375 - val_loss: 1340960.8750\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 738659.2500 - val_loss: 1387431.7500\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 770675.5625 - val_loss: 1075156.5000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 850272.2500 - val_loss: 1227609.5000\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 778693.3750 - val_loss: 1155816.8750\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 756250.6250 - val_loss: 1495547.7500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 799798.6250 - val_loss: 1444088.8750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 810477.6250 - val_loss: 1001005.2500\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 718086.7500 - val_loss: 1213924.3750\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 763054.2500 - val_loss: 1593227.5000\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 715728.2500 - val_loss: 1203830.7500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 626067.7500 - val_loss: 1192524.5000\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 691147.7500 - val_loss: 1122895.5000\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 653267.7500 - val_loss: 1077048.5000\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 669961.8750 - val_loss: 1408009.0000\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 707465.5625 - val_loss: 973068.2500\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 636395.1875 - val_loss: 1439755.1250\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 688809.6250 - val_loss: 1128357.7500\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 606217.2500 - val_loss: 1151656.5000\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 732628.0000 - val_loss: 1249763.2500\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 629554.3750 - val_loss: 1262843.7500\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 584761.2500 - val_loss: 1212071.8750\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 671046.3750 - val_loss: 1313224.7500\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 731060.7500 - val_loss: 1226784.7500\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 622804.4375 - val_loss: 1120823.6250\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 661015.5625 - val_loss: 1126805.1250\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 46781772.0000 - val_loss: 22464322.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 17302914.0000 - val_loss: 14709696.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11884299.0000 - val_loss: 12743370.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8898087.0000 - val_loss: 9122192.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6811628.5000 - val_loss: 6840841.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5325886.0000 - val_loss: 5605838.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4292635.0000 - val_loss: 4864864.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3724585.0000 - val_loss: 4542237.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2916656.5000 - val_loss: 4215418.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2745380.5000 - val_loss: 3405159.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2382456.5000 - val_loss: 3738939.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2299880.2500 - val_loss: 2678770.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1941850.5000 - val_loss: 2995147.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1760525.7500 - val_loss: 2715846.2500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1757832.6250 - val_loss: 2304985.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1573076.7500 - val_loss: 2218830.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1367124.5000 - val_loss: 2428227.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1377780.5000 - val_loss: 1777903.3750\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1281819.3750 - val_loss: 2013937.8750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1081594.5000 - val_loss: 1677248.6250\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1014469.2500 - val_loss: 1607238.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1005534.8750 - val_loss: 1721267.8750\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1006799.8750 - val_loss: 1540266.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 942406.8750 - val_loss: 1447246.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 845792.5625 - val_loss: 1604645.7500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 926243.6875 - val_loss: 1262876.0000\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 840339.1250 - val_loss: 1303238.1250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 744473.6875 - val_loss: 1275713.2500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 780139.7500 - val_loss: 1255259.1250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 791241.6250 - val_loss: 1475391.0000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 750235.5625 - val_loss: 1460078.0000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 731784.1250 - val_loss: 1454931.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 710145.9375 - val_loss: 1358794.8750\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 666975.3125 - val_loss: 1345776.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 664144.3125 - val_loss: 1072874.7500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 662024.1875 - val_loss: 1360706.5000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 713765.5625 - val_loss: 1138459.1250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 742964.5625 - val_loss: 1459024.3750\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 736252.8125 - val_loss: 1212879.0000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 614315.0625 - val_loss: 1510064.5000\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 634720.2500 - val_loss: 1252833.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 767403.7500 - val_loss: 1675801.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 796847.6875 - val_loss: 1203866.0000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 612263.9375 - val_loss: 980730.2500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 569770.2500 - val_loss: 1298662.1250\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 779339.3750 - val_loss: 1243927.2500\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 686825.3125 - val_loss: 1012105.6250\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 608979.5000 - val_loss: 1069193.1250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 579049.2500 - val_loss: 1142626.8750\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 636707.7500 - val_loss: 1070846.6250\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 751136.6250 - val_loss: 1506253.7500\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 643346.2500 - val_loss: 1280317.1250\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 829477.0625 - val_loss: 1206565.1250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 818518.1875 - val_loss: 1330699.3750\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 49629648.0000 - val_loss: 23034780.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 19153360.0000 - val_loss: 16935366.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 13125625.0000 - val_loss: 12391643.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10071223.0000 - val_loss: 9965694.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8109792.0000 - val_loss: 9819008.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6694343.0000 - val_loss: 6989886.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5637310.0000 - val_loss: 6212688.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4696345.5000 - val_loss: 6677707.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4449896.5000 - val_loss: 4862129.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3778467.5000 - val_loss: 4432799.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3255270.5000 - val_loss: 3950635.7500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3131494.0000 - val_loss: 3498020.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2569693.2500 - val_loss: 3296635.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2230189.2500 - val_loss: 3079239.7500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2093249.7500 - val_loss: 2531936.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2064049.3750 - val_loss: 2466440.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1845235.6250 - val_loss: 2466669.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1864105.6250 - val_loss: 2731941.7500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1707971.7500 - val_loss: 2000045.0000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1507924.5000 - val_loss: 1726330.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1554911.1250 - val_loss: 2190450.2500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1537924.6250 - val_loss: 2463076.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1402931.5000 - val_loss: 2224014.5000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1259305.5000 - val_loss: 1874885.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1188476.7500 - val_loss: 1438874.0000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1063935.3750 - val_loss: 1730536.6250\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1026806.3125 - val_loss: 1633467.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1050080.7500 - val_loss: 1986730.6250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 994020.2500 - val_loss: 1485921.0000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 859189.1875 - val_loss: 1397737.5000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 941603.6875 - val_loss: 1220140.7500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 932179.7500 - val_loss: 1654110.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 921167.5000 - val_loss: 1498898.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 825444.1250 - val_loss: 1384498.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 901402.5625 - val_loss: 1633159.6250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 896629.0000 - val_loss: 1466970.8750\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 901094.7500 - val_loss: 1476603.1250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 819738.5625 - val_loss: 1235047.0000\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 797011.2500 - val_loss: 1482525.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 808637.3125 - val_loss: 1254828.1250\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 663140.0625 - val_loss: 1111288.5000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 735593.5625 - val_loss: 1556879.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 695286.7500 - val_loss: 1220898.0000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 674648.8750 - val_loss: 1046087.3750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 672004.3125 - val_loss: 1154708.7500\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 662524.6875 - val_loss: 1350950.8750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 662931.3750 - val_loss: 1173881.2500\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 611252.8750 - val_loss: 1266331.6250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 681308.8125 - val_loss: 1474403.0000\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 719170.0625 - val_loss: 1083486.6250\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 611669.8750 - val_loss: 1013003.0625\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 710708.3750 - val_loss: 1108310.3750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 566747.6250 - val_loss: 985498.9375\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 670349.1875 - val_loss: 1251539.3750\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 672870.8750 - val_loss: 1168111.7500\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 637724.0000 - val_loss: 1233150.0000\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 679748.2500 - val_loss: 1251257.2500\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 645987.4375 - val_loss: 1305850.5000\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 632223.2500 - val_loss: 954926.3750\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 556911.4375 - val_loss: 1033541.1875\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 615097.3125 - val_loss: 1344999.7500\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 717412.4375 - val_loss: 1119206.1250\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 633741.8750 - val_loss: 1063440.5000\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 698678.3125 - val_loss: 1305221.1250\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 674447.6875 - val_loss: 1138793.1250\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 611688.0000 - val_loss: 940172.0000\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 652591.5625 - val_loss: 970841.3750\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 597153.3125 - val_loss: 1053105.7500\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 605462.3125 - val_loss: 1195442.5000\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 648852.5625 - val_loss: 1299993.7500\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 705191.6875 - val_loss: 1102805.6250\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 512247.8750 - val_loss: 963087.6875\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 577327.5625 - val_loss: 1137859.2500\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 630796.5000 - val_loss: 1105322.8750\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 588413.6250 - val_loss: 1290065.0000\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 711205.5000 - val_loss: 1261548.7500\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 45124200.0000 - val_loss: 21621420.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 18291682.0000 - val_loss: 14431003.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 13132789.0000 - val_loss: 11290198.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9515291.0000 - val_loss: 9280529.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7730107.5000 - val_loss: 7230093.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6549906.0000 - val_loss: 6959527.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5899506.5000 - val_loss: 7169449.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5494791.5000 - val_loss: 5979187.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4347844.5000 - val_loss: 4619314.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3891419.0000 - val_loss: 4442763.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3301807.0000 - val_loss: 3605900.7500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3200093.2500 - val_loss: 3758942.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2648366.7500 - val_loss: 3063766.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2682124.5000 - val_loss: 3340830.2500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2288677.7500 - val_loss: 3266664.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2338642.5000 - val_loss: 2765606.2500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2222376.0000 - val_loss: 2963355.2500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1913628.6250 - val_loss: 2353424.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1729411.5000 - val_loss: 2121766.7500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1669430.5000 - val_loss: 2531201.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1609492.6250 - val_loss: 2518464.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1523031.1250 - val_loss: 2122114.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1320018.8750 - val_loss: 1996325.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1281586.3750 - val_loss: 1583246.6250\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1375994.3750 - val_loss: 1761222.8750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1287635.6250 - val_loss: 1719094.1250\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1046479.3125 - val_loss: 1833245.2500\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1031138.6875 - val_loss: 1362339.8750\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1080298.7500 - val_loss: 2110079.5000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 959126.6875 - val_loss: 1387166.6250\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 992114.1250 - val_loss: 1457936.0000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 883327.2500 - val_loss: 1370256.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 869843.2500 - val_loss: 1359738.8750\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1053677.5000 - val_loss: 1482137.6250\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 802937.9375 - val_loss: 1295027.7500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 743704.8750 - val_loss: 1179197.1250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 782946.0000 - val_loss: 1268659.2500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 764889.6250 - val_loss: 1250684.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 750170.3750 - val_loss: 1216612.3750\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 766842.7500 - val_loss: 1351939.5000\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 718466.0000 - val_loss: 957395.4375\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 774457.8125 - val_loss: 1282184.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 811201.2500 - val_loss: 1817637.5000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 718944.1250 - val_loss: 1102516.2500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 808629.2500 - val_loss: 1063839.5000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 687344.1875 - val_loss: 1427874.3750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 766399.6875 - val_loss: 1206003.2500\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 641055.7500 - val_loss: 884940.5625\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 619984.7500 - val_loss: 1451381.5000\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 688155.0000 - val_loss: 1124018.3750\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 682386.1875 - val_loss: 1366680.0000\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 733954.7500 - val_loss: 1079112.6250\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 638152.5625 - val_loss: 1239899.2500\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 700432.2500 - val_loss: 986119.6250\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 629112.6250 - val_loss: 1334921.7500\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 694342.6250 - val_loss: 1084468.7500\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 615477.3750 - val_loss: 1391695.3750\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 655601.0000 - val_loss: 906455.7500\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 61672788.0000 - val_loss: 29658162.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 21553884.0000 - val_loss: 18628232.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 14739751.0000 - val_loss: 14660834.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11534025.0000 - val_loss: 11335676.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 9146060.0000 - val_loss: 9016723.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8071637.5000 - val_loss: 7305922.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6127725.0000 - val_loss: 6738082.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5492675.0000 - val_loss: 6084502.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4694279.5000 - val_loss: 6116794.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4041437.0000 - val_loss: 3971824.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3541471.7500 - val_loss: 3923218.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2993219.7500 - val_loss: 3757132.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2943069.7500 - val_loss: 3569504.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2737414.0000 - val_loss: 3549808.7500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2301388.2500 - val_loss: 3199931.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2180127.2500 - val_loss: 3222217.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2007251.1250 - val_loss: 2813162.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1812620.0000 - val_loss: 2178394.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1785925.1250 - val_loss: 2645282.0000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1760804.7500 - val_loss: 2023026.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1594740.6250 - val_loss: 2016930.8750\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1463746.5000 - val_loss: 1865018.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1565528.3750 - val_loss: 1825272.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1461534.1250 - val_loss: 2271863.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1206473.3750 - val_loss: 1678161.3750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1154222.7500 - val_loss: 2036403.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1191332.7500 - val_loss: 1901766.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1306064.5000 - val_loss: 1791019.7500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1043497.7500 - val_loss: 1675237.1250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1122174.0000 - val_loss: 1769650.3750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1137211.6250 - val_loss: 1628750.0000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 949084.2500 - val_loss: 1405177.6250\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 968075.8750 - val_loss: 1582064.0000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1101778.7500 - val_loss: 1829810.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 989063.7500 - val_loss: 1322857.7500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 862984.6875 - val_loss: 1552663.0000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 951202.0000 - val_loss: 1671691.3750\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 824716.6875 - val_loss: 1318837.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 839072.6875 - val_loss: 1447008.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 835952.6250 - val_loss: 1333125.8750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 754387.2500 - val_loss: 1426107.5000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 957165.1250 - val_loss: 1562570.0000\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 786600.5000 - val_loss: 1218483.2500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 760404.3750 - val_loss: 1250850.3750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 813441.1875 - val_loss: 1234402.3750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 774545.8750 - val_loss: 1513366.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 764083.2500 - val_loss: 1526137.7500\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 750513.4375 - val_loss: 902965.7500\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 733191.3750 - val_loss: 1338413.2500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 686640.5625 - val_loss: 1024210.0000\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 763407.3750 - val_loss: 1137407.5000\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 724712.2500 - val_loss: 1147673.0000\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 704298.7500 - val_loss: 1186275.8750\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 726094.8125 - val_loss: 1290591.5000\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 713608.0625 - val_loss: 1141933.8750\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 632107.8750 - val_loss: 1170719.2500\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 609439.0625 - val_loss: 1181812.3750\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 636630.3750 - val_loss: 1169597.3750\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 41797544.0000 - val_loss: 21451152.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 17817472.0000 - val_loss: 13245831.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 11697137.0000 - val_loss: 9954438.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8223800.0000 - val_loss: 9890759.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7277572.0000 - val_loss: 8621490.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5728656.5000 - val_loss: 5455690.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5044755.5000 - val_loss: 5817108.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4304196.0000 - val_loss: 4435593.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3549592.5000 - val_loss: 4139504.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3113559.0000 - val_loss: 3456113.2500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2970835.7500 - val_loss: 3003611.7500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2485832.0000 - val_loss: 2941352.7500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2367246.7500 - val_loss: 3060599.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2187708.5000 - val_loss: 3067299.2500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2095049.7500 - val_loss: 2489095.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1893545.7500 - val_loss: 2246395.2500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1743213.3750 - val_loss: 2423013.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1528249.2500 - val_loss: 2296697.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1550708.5000 - val_loss: 2302105.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1555062.6250 - val_loss: 2191706.2500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1434823.0000 - val_loss: 1856210.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1452819.5000 - val_loss: 1876152.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1219804.6250 - val_loss: 2222089.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1281664.0000 - val_loss: 1807242.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1156954.8750 - val_loss: 1759659.3750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1124984.3750 - val_loss: 1727521.3750\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1109000.0000 - val_loss: 1649650.5000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1002869.1250 - val_loss: 1426820.1250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 913210.2500 - val_loss: 1513708.5000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 891607.7500 - val_loss: 1504959.5000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 952747.0000 - val_loss: 1587754.1250\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 881107.5625 - val_loss: 1335940.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 763680.9375 - val_loss: 1334111.3750\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 733837.8750 - val_loss: 1438415.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 832841.2500 - val_loss: 1197123.6250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 726562.9375 - val_loss: 1065483.8750\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 804213.0000 - val_loss: 1471063.3750\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 775919.5625 - val_loss: 1484841.0000\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 712551.5000 - val_loss: 1760797.0000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 674884.9375 - val_loss: 944795.0625\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 809947.3125 - val_loss: 1332350.5000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 797731.2500 - val_loss: 1175038.3750\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 705283.4375 - val_loss: 1614064.7500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 754789.8125 - val_loss: 1299231.0000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 625361.4375 - val_loss: 1045202.3125\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 608918.3125 - val_loss: 1274096.1250\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 696815.6250 - val_loss: 1420437.1250\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 634018.1875 - val_loss: 1050327.5000\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 650331.8125 - val_loss: 1195495.6250\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 632877.5000 - val_loss: 1325453.5000\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 49885592.0000 - val_loss: 24441290.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 16168089.0000 - val_loss: 15849869.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 11380262.0000 - val_loss: 10866316.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8578623.0000 - val_loss: 7750851.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6858424.0000 - val_loss: 6574348.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5500859.0000 - val_loss: 6038367.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4552853.0000 - val_loss: 4874608.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3898460.0000 - val_loss: 3886016.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3146991.5000 - val_loss: 3445192.7500\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2831965.5000 - val_loss: 3196650.2500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2491847.2500 - val_loss: 2904927.7500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2336580.0000 - val_loss: 2608128.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2152176.0000 - val_loss: 2733084.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2021512.6250 - val_loss: 2959954.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1844111.2500 - val_loss: 2224607.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1574255.0000 - val_loss: 1780585.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1464491.7500 - val_loss: 2213102.2500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1460385.2500 - val_loss: 1985083.3750\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1399045.1250 - val_loss: 2021800.0000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1234568.6250 - val_loss: 1431355.5000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 985106.4375 - val_loss: 1455080.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 999270.1250 - val_loss: 1452583.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 905664.6875 - val_loss: 1590252.5000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 922839.0000 - val_loss: 1549088.0000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 873351.2500 - val_loss: 1619012.6250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 888465.2500 - val_loss: 1375959.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 700702.3750 - val_loss: 1262449.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 859098.6875 - val_loss: 1858828.6250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 860302.5625 - val_loss: 1321667.3750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 707047.5625 - val_loss: 1254557.3750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 771609.5625 - val_loss: 1402987.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 711941.5000 - val_loss: 1185894.0000\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 828376.4375 - val_loss: 1564267.5000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 711848.0000 - val_loss: 1010502.5000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 678458.7500 - val_loss: 990695.0625\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 748868.1875 - val_loss: 1136858.0000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 671615.5000 - val_loss: 1158734.3750\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 784722.0625 - val_loss: 1407601.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 705236.5625 - val_loss: 1216077.8750\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 704826.4375 - val_loss: 1267647.8750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 598648.2500 - val_loss: 1166282.3750\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 793010.0625 - val_loss: 1170050.3750\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 697540.0000 - val_loss: 1101353.3750\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 719163.8750 - val_loss: 1312670.6250\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 680950.2500 - val_loss: 1418162.6250\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 141.4640 - val_loss: 140.7697\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 136.7294 - val_loss: 136.7216\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136.4661 - val_loss: 137.5738\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136.9055 - val_loss: 136.8928\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 138.3728 - val_loss: 136.7313\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 137.9259 - val_loss: 136.7310\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 137.6275 - val_loss: 136.8177\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 137.0820 - val_loss: 139.1262\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136.7093 - val_loss: 134.6487\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.2464 - val_loss: 136.4989\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.6416 - val_loss: 134.1840\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.2114 - val_loss: 132.4987\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.4137 - val_loss: 136.0503\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.0308 - val_loss: 137.3337\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.8075 - val_loss: 134.1810\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.1783 - val_loss: 132.4311\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.0177 - val_loss: 131.2904\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4366 - val_loss: 128.6374\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6258 - val_loss: 127.3546\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.6349 - val_loss: 129.4314\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5924 - val_loss: 128.9165\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4968 - val_loss: 128.5731\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6955 - val_loss: 128.5496\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.3984 - val_loss: 129.4199\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.0411 - val_loss: 127.2446\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3258 - val_loss: 128.3909\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3484 - val_loss: 128.9650\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3865 - val_loss: 127.5731\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5122 - val_loss: 128.5646\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4401 - val_loss: 127.8751\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3676 - val_loss: 128.3577\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4546 - val_loss: 129.4680\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0448 - val_loss: 127.7232\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0187 - val_loss: 128.6001\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.3355 - val_loss: 128.4705\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 153.4737 - val_loss: 151.0544\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 151.0694 - val_loss: 152.6430\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 152.0995 - val_loss: 150.7353\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 151.8490 - val_loss: 152.4748\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 149.9653 - val_loss: 148.4247\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 148.4836 - val_loss: 145.7935\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 143.5108 - val_loss: 141.4074\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 139.9658 - val_loss: 140.7103\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 137.3983 - val_loss: 138.7722\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 134.1377 - val_loss: 132.0719\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.6082 - val_loss: 131.3045\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.1174 - val_loss: 131.4042\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.0891 - val_loss: 130.4831\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2245 - val_loss: 130.5336\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.5294 - val_loss: 127.5810\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.1492 - val_loss: 129.3294\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7795 - val_loss: 129.2086\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6822 - val_loss: 131.3206\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5770 - val_loss: 129.3631\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.1503 - val_loss: 128.3050\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0955 - val_loss: 128.7015\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0649 - val_loss: 129.3916\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4918 - val_loss: 127.9726\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3446 - val_loss: 127.6786\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9383 - val_loss: 127.4586\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.2241 - val_loss: 127.9795\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.2482 - val_loss: 130.4689\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.3435 - val_loss: 128.0544\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9250 - val_loss: 127.5696\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5218 - val_loss: 128.1801\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9404 - val_loss: 128.0908\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5054 - val_loss: 127.4179\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9597 - val_loss: 128.3806\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5357 - val_loss: 128.2146\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6227 - val_loss: 129.2992\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5972 - val_loss: 127.2588\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4329 - val_loss: 128.4718\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3198 - val_loss: 127.4841\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.2283 - val_loss: 127.8021\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8736 - val_loss: 128.1854\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.6419 - val_loss: 127.3818\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.2335 - val_loss: 128.9553\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8346 - val_loss: 127.6837\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.2601 - val_loss: 127.2059\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7533 - val_loss: 127.7211\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.6049 - val_loss: 127.8266\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.1393 - val_loss: 128.1484\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.9912 - val_loss: 127.2294\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.9532 - val_loss: 127.6628\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.9605 - val_loss: 127.3551\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.1478 - val_loss: 126.9793\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.2380 - val_loss: 127.3771\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.6745 - val_loss: 127.2337\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.6461 - val_loss: 127.7620\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.1652 - val_loss: 128.3522\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 119.5046 - val_loss: 127.0131\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.6366 - val_loss: 128.0862\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.2168 - val_loss: 127.8171\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 118.9035 - val_loss: 127.2719\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.1595 - val_loss: 127.8397\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 118.6603 - val_loss: 128.4894\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 151.9503 - val_loss: 149.7312\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 146.1120 - val_loss: 148.0641\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 143.1663 - val_loss: 135.0863\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.9655 - val_loss: 134.3240\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.1393 - val_loss: 134.6124\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.9784 - val_loss: 133.4180\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.5911 - val_loss: 133.1415\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9569 - val_loss: 135.0799\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.0124 - val_loss: 133.0907\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9907 - val_loss: 133.5264\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9068 - val_loss: 133.0431\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.4775 - val_loss: 134.7776\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.8224 - val_loss: 132.9231\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.5465 - val_loss: 134.3573\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.7517 - val_loss: 132.3555\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.4499 - val_loss: 132.3619\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.8377 - val_loss: 133.9849\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.3943 - val_loss: 132.8002\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.8630 - val_loss: 131.8158\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.7927 - val_loss: 130.8736\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5943 - val_loss: 128.7847\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.1901 - val_loss: 128.8995\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2828 - val_loss: 129.4358\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9549 - val_loss: 130.9005\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9671 - val_loss: 129.5581\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7445 - val_loss: 130.2474\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.1112 - val_loss: 129.5785\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9986 - val_loss: 130.1143\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.2485 - val_loss: 129.9472\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9375 - val_loss: 129.4089\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5922 - val_loss: 128.5717\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8789 - val_loss: 130.2239\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4614 - val_loss: 129.4286\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9708 - val_loss: 127.8564\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5407 - val_loss: 129.7698\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4275 - val_loss: 128.3732\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.7610 - val_loss: 128.9711\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.1887 - val_loss: 128.6940\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.8630 - val_loss: 130.4308\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.1632 - val_loss: 129.7491\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.9299 - val_loss: 128.1214\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8017 - val_loss: 130.2309\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8484 - val_loss: 131.0515\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.9865 - val_loss: 128.2053\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 142.4131 - val_loss: 139.5791\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.5748 - val_loss: 134.2836\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9851 - val_loss: 134.8379\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.8265 - val_loss: 132.4296\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.9895 - val_loss: 132.0116\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.0143 - val_loss: 132.6775\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.0987 - val_loss: 134.2172\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.6273 - val_loss: 129.8824\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7224 - val_loss: 131.2444\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.1462 - val_loss: 130.9602\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.2954 - val_loss: 131.0459\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5087 - val_loss: 131.7279\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.8941 - val_loss: 128.9072\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.3179 - val_loss: 130.5812\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9713 - val_loss: 129.6662\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4464 - val_loss: 129.7709\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.7888 - val_loss: 129.3046\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.3120 - val_loss: 128.9500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.3498 - val_loss: 129.1388\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.3173 - val_loss: 128.9679\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9362 - val_loss: 127.7449\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.2134 - val_loss: 129.8226\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.2143 - val_loss: 128.3154\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7961 - val_loss: 127.6151\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.1662 - val_loss: 128.3313\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0064 - val_loss: 128.9732\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.0111 - val_loss: 127.5277\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0945 - val_loss: 129.3053\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7242 - val_loss: 129.4433\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5514 - val_loss: 128.4567\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9757 - val_loss: 129.5724\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.2528 - val_loss: 128.2253\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.7802 - val_loss: 127.3941\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9156 - val_loss: 129.0076\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9945 - val_loss: 129.1426\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.7844 - val_loss: 127.6891\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4485 - val_loss: 127.3885\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0539 - val_loss: 127.9888\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4565 - val_loss: 128.4471\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.8711 - val_loss: 128.1978\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.2218 - val_loss: 129.8652\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.9172 - val_loss: 127.9148\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.4795 - val_loss: 128.3378\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.0336 - val_loss: 128.2563\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7839 - val_loss: 129.3224\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.6849 - val_loss: 127.7544\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.3593 - val_loss: 127.4028\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 140.1975 - val_loss: 134.9124\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.5318 - val_loss: 136.3528\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.9229 - val_loss: 133.3031\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 134.7482 - val_loss: 135.5284\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.3392 - val_loss: 135.3749\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.9608 - val_loss: 134.5046\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.7435 - val_loss: 133.8554\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.3769 - val_loss: 134.8492\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.0454 - val_loss: 137.1561\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.7889 - val_loss: 132.5623\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.1043 - val_loss: 132.1084\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9333 - val_loss: 133.4468\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.7899 - val_loss: 134.0041\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.5143 - val_loss: 132.9487\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9691 - val_loss: 136.0458\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.3179 - val_loss: 133.7930\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.5797 - val_loss: 134.7472\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.2779 - val_loss: 133.1896\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.6561 - val_loss: 131.6219\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.5457 - val_loss: 131.4452\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.3280 - val_loss: 132.7045\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.8608 - val_loss: 132.3099\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.6624 - val_loss: 131.1303\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.9995 - val_loss: 132.8198\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.6362 - val_loss: 132.7350\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.7885 - val_loss: 132.6707\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.5366 - val_loss: 131.2285\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.1071 - val_loss: 131.5296\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.6170 - val_loss: 130.3945\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.9059 - val_loss: 130.1978\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.6031 - val_loss: 132.1358\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3386 - val_loss: 131.9824\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.1223 - val_loss: 132.1541\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9687 - val_loss: 131.0368\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.8608 - val_loss: 130.2135\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.5807 - val_loss: 131.4926\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.0220 - val_loss: 130.6823\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.1783 - val_loss: 130.5567\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2143 - val_loss: 131.0277\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.8753 - val_loss: 131.1496\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 139.6370 - val_loss: 135.4842\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.1524 - val_loss: 134.6543\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.5892 - val_loss: 136.7836\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.9189 - val_loss: 133.4992\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.6092 - val_loss: 135.8620\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.5862 - val_loss: 133.3696\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.1805 - val_loss: 134.5452\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.6133 - val_loss: 133.3204\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.4500 - val_loss: 132.2796\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.0563 - val_loss: 133.9707\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.4732 - val_loss: 132.6896\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.4686 - val_loss: 132.3727\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.2411 - val_loss: 130.6807\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.0192 - val_loss: 133.4543\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.6375 - val_loss: 131.5871\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.0485 - val_loss: 132.5717\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.7559 - val_loss: 130.3533\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.0200 - val_loss: 129.4693\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.7357 - val_loss: 130.1110\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.1523 - val_loss: 128.2044\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.0314 - val_loss: 131.1502\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.2164 - val_loss: 129.2931\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2514 - val_loss: 129.5823\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.3936 - val_loss: 129.6752\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.5712 - val_loss: 128.1119\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5907 - val_loss: 129.5659\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.6738 - val_loss: 128.9543\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0941 - val_loss: 128.6856\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6310 - val_loss: 129.2783\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7462 - val_loss: 128.5609\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6886 - val_loss: 128.0132\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0712 - val_loss: 128.8074\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3598 - val_loss: 129.4827\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.6189 - val_loss: 127.1369\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3412 - val_loss: 126.8950\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5839 - val_loss: 129.8493\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.1281 - val_loss: 128.5022\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4121 - val_loss: 129.3776\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5661 - val_loss: 128.1177\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7293 - val_loss: 127.4908\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.1162 - val_loss: 129.7973\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.2143 - val_loss: 127.8775\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.5239 - val_loss: 126.9526\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.1049 - val_loss: 127.5063\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.3450 - val_loss: 127.3456\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 151.3012 - val_loss: 151.3374\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 148.1459 - val_loss: 149.0682\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 145.9225 - val_loss: 145.2298\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 145.5756 - val_loss: 148.7857\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 147.9496 - val_loss: 149.3711\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 145.9199 - val_loss: 146.1801\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 145.3678 - val_loss: 146.3536\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 145.3751 - val_loss: 146.5774\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 141.9737 - val_loss: 143.8148\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 137.4551 - val_loss: 140.3363\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136.9035 - val_loss: 138.5504\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136.8618 - val_loss: 139.1612\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.1978 - val_loss: 139.5419\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 135.1287 - val_loss: 137.4257\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.0955 - val_loss: 138.3542\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.0046 - val_loss: 139.0243\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 135.2990 - val_loss: 137.0454\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.8255 - val_loss: 134.3808\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.9265 - val_loss: 131.7606\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.0430 - val_loss: 131.6584\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.9157 - val_loss: 131.8499\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.4293 - val_loss: 131.3228\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.3338 - val_loss: 130.5398\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.8872 - val_loss: 131.1973\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.1579 - val_loss: 129.2935\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8402 - val_loss: 130.3238\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5924 - val_loss: 128.9845\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8807 - val_loss: 129.7149\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9472 - val_loss: 129.7302\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.6324 - val_loss: 129.1559\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.6259 - val_loss: 129.2698\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3149 - val_loss: 128.3033\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7338 - val_loss: 130.1042\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.2419 - val_loss: 128.1960\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.2341 - val_loss: 128.9930\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6830 - val_loss: 128.0758\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6036 - val_loss: 128.1529\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.0124 - val_loss: 128.3891\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3165 - val_loss: 128.5503\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.6424 - val_loss: 128.5438\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.6739 - val_loss: 128.2867\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.1595 - val_loss: 128.7508\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.5443 - val_loss: 129.4113\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.2592 - val_loss: 128.0319\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.3580 - val_loss: 129.7045\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.6561 - val_loss: 128.3279\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.3269 - val_loss: 128.1344\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.6683 - val_loss: 129.7393\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.1820 - val_loss: 128.3457\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.0342 - val_loss: 129.0188\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.1928 - val_loss: 127.8183\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.4442 - val_loss: 129.5843\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.8893 - val_loss: 128.9088\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.1477 - val_loss: 129.2241\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.3989 - val_loss: 129.8213\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.7598 - val_loss: 129.1719\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 118.9566 - val_loss: 129.6333\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 118.9630 - val_loss: 129.6086\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.5916 - val_loss: 129.7048\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 118.7750 - val_loss: 129.8926\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 118.8380 - val_loss: 128.5089\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 147.6487 - val_loss: 144.3522\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 137.7565 - val_loss: 140.5474\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 134.0344 - val_loss: 135.9324\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.0979 - val_loss: 135.0889\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.7964 - val_loss: 133.9730\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.2693 - val_loss: 136.1263\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.0671 - val_loss: 132.9456\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.6214 - val_loss: 132.4906\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.0960 - val_loss: 135.2402\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.9229 - val_loss: 136.7470\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.3487 - val_loss: 135.5583\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.0092 - val_loss: 131.5530\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.6683 - val_loss: 132.8807\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.9967 - val_loss: 131.8033\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.8192 - val_loss: 130.9891\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.9425 - val_loss: 133.1899\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.9398 - val_loss: 130.7272\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2084 - val_loss: 127.9386\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.6487 - val_loss: 131.2617\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.3133 - val_loss: 129.5509\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.7646 - val_loss: 128.5591\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6814 - val_loss: 130.2637\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.2823 - val_loss: 129.8124\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.8231 - val_loss: 129.2002\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6324 - val_loss: 129.5044\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8523 - val_loss: 128.2312\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.1685 - val_loss: 128.8799\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.7505 - val_loss: 128.1643\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 143.1324 - val_loss: 145.6708\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 141.7253 - val_loss: 143.1828\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 140.2742 - val_loss: 141.5104\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 143.0597 - val_loss: 146.4672\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 141.9619 - val_loss: 142.9104\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 139.6427 - val_loss: 140.8631\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 139.1626 - val_loss: 142.4881\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 139.7504 - val_loss: 141.7571\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.1616 - val_loss: 144.0678\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 140.6170 - val_loss: 142.5040\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 138.2441 - val_loss: 136.7489\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.4315 - val_loss: 133.9068\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.1030 - val_loss: 133.4084\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.2487 - val_loss: 137.4328\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.0507 - val_loss: 132.7762\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.5579 - val_loss: 133.7914\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.5011 - val_loss: 131.9703\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.2017 - val_loss: 132.9119\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.0169 - val_loss: 132.1545\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.3714 - val_loss: 134.5198\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.5703 - val_loss: 134.8070\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.1629 - val_loss: 132.6569\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.6834 - val_loss: 133.8530\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.9283 - val_loss: 132.0209\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.5156 - val_loss: 133.7725\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.5227 - val_loss: 131.8597\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.9644 - val_loss: 132.3636\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.0900 - val_loss: 133.4066\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3712 - val_loss: 132.1567\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.3256 - val_loss: 131.1767\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6882 - val_loss: 131.7888\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5285 - val_loss: 131.2089\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.5017 - val_loss: 130.1138\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9102 - val_loss: 128.5591\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5213 - val_loss: 128.8799\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5665 - val_loss: 129.5650\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3950 - val_loss: 129.2875\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6235 - val_loss: 128.8621\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7245 - val_loss: 128.9358\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8393 - val_loss: 131.1107\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.4760 - val_loss: 128.3793\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.3796 - val_loss: 131.5773\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.4954 - val_loss: 128.7670\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.6606 - val_loss: 129.8651\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.7250 - val_loss: 129.8288\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.6256 - val_loss: 128.4300\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.7284 - val_loss: 128.0274\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.9530 - val_loss: 129.9423\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.5904 - val_loss: 129.5110\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.3675 - val_loss: 129.6618\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120.8342 - val_loss: 128.2075\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.8905 - val_loss: 129.0656\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.0498 - val_loss: 130.4953\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 119.3870 - val_loss: 127.9262\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 119.7618 - val_loss: 127.6065\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.2412 - val_loss: 128.7446\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.8738 - val_loss: 129.9727\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 118.8215 - val_loss: 129.2510\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.1121 - val_loss: 130.3590\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.5614 - val_loss: 130.7809\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 119.3791 - val_loss: 129.2700\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 118.9288 - val_loss: 128.6731\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 118.4524 - val_loss: 128.6499\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 118.5234 - val_loss: 128.4053\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 118.3598 - val_loss: 129.1999\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 6ms/step - loss: 140.9327 - val_loss: 139.3168\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 138.3137 - val_loss: 141.1756\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 137.0119 - val_loss: 137.7240\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.1038 - val_loss: 137.7358\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 137.9233 - val_loss: 140.5915\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.0211 - val_loss: 137.0749\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.4604 - val_loss: 135.9691\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.5506 - val_loss: 135.7363\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.6566 - val_loss: 138.9397\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.0230 - val_loss: 136.3814\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.8513 - val_loss: 135.3582\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.6528 - val_loss: 137.2228\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.5113 - val_loss: 135.9967\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.8185 - val_loss: 136.8878\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.5873 - val_loss: 137.0697\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.7767 - val_loss: 136.6244\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.3731 - val_loss: 136.6853\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.7689 - val_loss: 136.9014\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.3357 - val_loss: 136.1521\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.1047 - val_loss: 136.7212\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.2068 - val_loss: 137.2035\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "mase_models_G = train_bagging_models_G(model_num, MASE(y_train,24),300,10,8,0.001)\n",
    "mape_models_G = train_bagging_models_G(model_num,'mape',300,10,8,0.001)\n",
    "smape_models_G = train_bagging_models_G(model_num, SMAPE(),300,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8e382f-5a29-464f-abe0-eeee880e371b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2244488404387937, 0.24109146095902362)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1,_=smape_models_G\n",
    "pred2,_=mase_models_G\n",
    "pred3,_=mape_models_G\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2, test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3, test_X)\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "mean_squared_error(test_y.flatten(),fin_pred_G.flatten()),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed0c5ab-e5ea-4c88-b295-0002063f1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(fin_pred_G.reshape(-1,24)).to_csv(\"../result7_new/NBEATs_B/pred_mid_G.csv\")\n",
    "for i in range(10):\n",
    "    pd.DataFrame(concat_G[i].reshape(-1,24)).to_csv(f\"../result7_new/NBEATs_B/pred_G{i}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
