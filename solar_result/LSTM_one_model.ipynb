{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576d6b0f-873c-4751-8965-0dc5b9d95533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1],y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps,unit):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(unit, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -output_timesteps:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=0, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455d69cb-a9d9-479b-9b05-81cb73c53b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 16:41:16.900604: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-09-06 16:41:16.900645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-09-06 16:41:16.901323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 67: early stopping\n",
      "'########################################################Model0\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 18: early stopping\n",
      "'########################################################Model0\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model0\n",
      "12/12 [==============================] - 0s 9ms/step\n",
      "12/12 [==============================] - 0s 10ms/step\n",
      "12/12 [==============================] - 0s 10ms/step\n",
      "0.26318839055946364 0.2599580355666313\n",
      "0.23907690662905057 0.24007488760818146\n",
      "1.165172431542645 0.594461878259684\n"
     ]
    }
   ],
   "source": [
    "target_X= pd.read_csv(\"data/solor_train_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "target_y =pd.read_csv(\"data/solor_train_output_7.csv\").iloc[:,1:].values\n",
    "test_X= pd.read_csv(\"data/solor_val_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "test_y =pd.read_csv(\"data/solor_val_output_7.csv\").iloc[:,1:].values\n",
    "\n",
    "X_train=target_X\n",
    "y_train=target_y\n",
    "\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]\n",
    "\n",
    "model_num = 1\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),300,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',300,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),300,10,8,0.001)\n",
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "a1= bagging_predict(pred1, test_X)\n",
    "a2= bagging_predict(pred2, test_X)\n",
    "a3= bagging_predict(pred3, test_X)\n",
    "\n",
    "print(mean_squared_error(test_y.flatten(),a1.flatten()),mean_absolute_error(test_y.flatten(),a1.flatten()))\n",
    "print(mean_squared_error(test_y.flatten(),a2.flatten()),mean_absolute_error(test_y.flatten(),a2.flatten()))\n",
    "print(mean_squared_error(test_y.flatten(),a3.flatten()),mean_absolute_error(test_y.flatten(),a3.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb34b81-96d5-4ee9-9618-b6f1faaca914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model0\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 22: early stopping\n",
      "'########################################################Model0\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model0\n",
      "12/12 [==============================] - 0s 8ms/step\n",
      "12/12 [==============================] - 0s 7ms/step\n",
      "12/12 [==============================] - 0s 7ms/step\n",
      "0.24838947126064972 0.24983518782090847\n",
      "0.2379673152935491 0.24469310761919746\n",
      "1.1892831134111048 0.6000721486582357\n"
     ]
    }
   ],
   "source": [
    "target_X= pd.read_csv(\"data/solor_train_input_5.csv\").iloc[:,(1+24*0):].values\n",
    "target_y =pd.read_csv(\"data/solor_train_output_5.csv\").iloc[:,1:].values\n",
    "test_X= pd.read_csv(\"data/solor_val_input_5.csv\").iloc[:,(1+24*0):].values\n",
    "test_y =pd.read_csv(\"data/solor_val_output_5.csv\").iloc[:,1:].values\n",
    "\n",
    "X_train=target_X\n",
    "y_train=target_y\n",
    "\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]\n",
    "\n",
    "model_num = 1\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),300,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',300,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),300,10,8,0.001)\n",
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "a1= bagging_predict(pred1, test_X)\n",
    "a2= bagging_predict(pred2, test_X)\n",
    "a3= bagging_predict(pred3, test_X)\n",
    "\n",
    "print(mean_squared_error(test_y.flatten(),a1.flatten()),mean_absolute_error(test_y.flatten(),a1.flatten()))\n",
    "print(mean_squared_error(test_y.flatten(),a2.flatten()),mean_absolute_error(test_y.flatten(),a2.flatten()))\n",
    "print(mean_squared_error(test_y.flatten(),a3.flatten()),mean_absolute_error(test_y.flatten(),a3.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54650517-8b68-4228-a7f4-85f10c37f03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model0\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 22: early stopping\n",
      "'########################################################Model0\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model0\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "0.27103444103985674 0.2643563491893518\n",
      "0.24107069477816717 0.2443009433545586\n",
      "1.1832813881170485 0.5982839716911494\n"
     ]
    }
   ],
   "source": [
    "target_X= pd.read_csv(\"data/solor_train_input_3.csv\").iloc[:,(1+24*0):].values\n",
    "target_y =pd.read_csv(\"data/solor_train_output_3.csv\").iloc[:,1:].values\n",
    "test_X= pd.read_csv(\"data/solor_val_input_3.csv\").iloc[:,(1+24*0):].values\n",
    "test_y =pd.read_csv(\"data/solor_val_output_3.csv\").iloc[:,1:].values\n",
    "\n",
    "X_train=target_X\n",
    "y_train=target_y\n",
    "\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]\n",
    "\n",
    "model_num = 1\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),300,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',300,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),300,10,8,0.001)\n",
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "a1= bagging_predict(pred1, test_X)\n",
    "a2= bagging_predict(pred2, test_X)\n",
    "a3= bagging_predict(pred3, test_X)\n",
    "\n",
    "print(mean_squared_error(test_y.flatten(),a1.flatten()),mean_absolute_error(test_y.flatten(),a1.flatten()))\n",
    "print(mean_squared_error(test_y.flatten(),a2.flatten()),mean_absolute_error(test_y.flatten(),a2.flatten()))\n",
    "print(mean_squared_error(test_y.flatten(),a3.flatten()),mean_absolute_error(test_y.flatten(),a3.flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
